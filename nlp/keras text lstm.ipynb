{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,roc_auc_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6851,) (762,) (6851,) (762,) (3263,)\n"
     ]
    }
   ],
   "source": [
    "train= pd.read_csv('./nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('./nlp-getting-started/test.csv')\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, train.target.values, \n",
    "                                                  stratify=train.target.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "xtest=test.text.values\n",
    "\n",
    "print(xtrain.shape, xvalid.shape, ytrain.shape, yvalid.shape,xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid) + list(xtest))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "xtest_seq = token.texts_to_sequences(xtest)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index\n",
    "\n",
    "\n",
    "ytrain_enc = keras.utils.to_categorical(ytrain)\n",
    "yvalid_enc = keras.utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[    0,     0,     0, ...,  1591,   131,    97],\n",
       "        [    0,     0,     0, ...,   664,  1596,   275],\n",
       "        [    0,     0,     0, ...,   497,   100,    42],\n",
       "        ...,\n",
       "        [    0,     0,     0, ...,     1,     2, 29319],\n",
       "        [    0,     0,     0, ...,     1,     2,  6123],\n",
       "        [    0,     0,     0, ...,    75,   259,  3090]]),\n",
       " {'t': 1,\n",
       "  'co': 2,\n",
       "  'http': 3,\n",
       "  'the': 4,\n",
       "  'a': 5,\n",
       "  'to': 6,\n",
       "  'in': 7,\n",
       "  'of': 8,\n",
       "  'and': 9,\n",
       "  'i': 10,\n",
       "  'is': 11,\n",
       "  'for': 12,\n",
       "  'on': 13,\n",
       "  'you': 14,\n",
       "  'my': 15,\n",
       "  'it': 16,\n",
       "  'with': 17,\n",
       "  'that': 18,\n",
       "  'by': 19,\n",
       "  'at': 20,\n",
       "  'this': 21,\n",
       "  'https': 22,\n",
       "  'from': 23,\n",
       "  'are': 24,\n",
       "  'be': 25,\n",
       "  'was': 26,\n",
       "  'amp': 27,\n",
       "  'have': 28,\n",
       "  'like': 29,\n",
       "  'as': 30,\n",
       "  'up': 31,\n",
       "  'just': 32,\n",
       "  'me': 33,\n",
       "  'but': 34,\n",
       "  'not': 35,\n",
       "  'so': 36,\n",
       "  'your': 37,\n",
       "  'out': 38,\n",
       "  'no': 39,\n",
       "  'will': 40,\n",
       "  'after': 41,\n",
       "  'all': 42,\n",
       "  'fire': 43,\n",
       "  'an': 44,\n",
       "  'we': 45,\n",
       "  'when': 46,\n",
       "  'if': 47,\n",
       "  \"i'm\": 48,\n",
       "  'has': 49,\n",
       "  'get': 50,\n",
       "  'new': 51,\n",
       "  'via': 52,\n",
       "  'more': 53,\n",
       "  'about': 54,\n",
       "  '2': 55,\n",
       "  'now': 56,\n",
       "  'or': 57,\n",
       "  'news': 58,\n",
       "  'he': 59,\n",
       "  'how': 60,\n",
       "  'people': 61,\n",
       "  'one': 62,\n",
       "  'they': 63,\n",
       "  'what': 64,\n",
       "  \"it's\": 65,\n",
       "  'who': 66,\n",
       "  'over': 67,\n",
       "  'been': 68,\n",
       "  'do': 69,\n",
       "  'into': 70,\n",
       "  \"'\": 71,\n",
       "  'can': 72,\n",
       "  \"don't\": 73,\n",
       "  'video': 74,\n",
       "  'emergency': 75,\n",
       "  'disaster': 76,\n",
       "  'there': 77,\n",
       "  'would': 78,\n",
       "  '3': 79,\n",
       "  'police': 80,\n",
       "  'her': 81,\n",
       "  'his': 82,\n",
       "  'than': 83,\n",
       "  'u': 84,\n",
       "  'still': 85,\n",
       "  'were': 86,\n",
       "  'some': 87,\n",
       "  'us': 88,\n",
       "  'why': 89,\n",
       "  'body': 90,\n",
       "  '1': 91,\n",
       "  'off': 92,\n",
       "  'burning': 93,\n",
       "  'rt': 94,\n",
       "  'suicide': 95,\n",
       "  'storm': 96,\n",
       "  'crash': 97,\n",
       "  'first': 98,\n",
       "  'time': 99,\n",
       "  'them': 100,\n",
       "  'had': 101,\n",
       "  'got': 102,\n",
       "  'back': 103,\n",
       "  'attack': 104,\n",
       "  'day': 105,\n",
       "  'know': 106,\n",
       "  'fires': 107,\n",
       "  'buildings': 108,\n",
       "  'two': 109,\n",
       "  's': 110,\n",
       "  'going': 111,\n",
       "  'california': 112,\n",
       "  'see': 113,\n",
       "  'our': 114,\n",
       "  'man': 115,\n",
       "  'bomb': 116,\n",
       "  'nuclear': 117,\n",
       "  'love': 118,\n",
       "  'world': 119,\n",
       "  'full': 120,\n",
       "  'year': 121,\n",
       "  'hiroshima': 122,\n",
       "  'go': 123,\n",
       "  'dead': 124,\n",
       "  '5': 125,\n",
       "  'watch': 126,\n",
       "  'youtube': 127,\n",
       "  'today': 128,\n",
       "  'life': 129,\n",
       "  'old': 130,\n",
       "  'car': 131,\n",
       "  'their': 132,\n",
       "  'killed': 133,\n",
       "  'train': 134,\n",
       "  'think': 135,\n",
       "  'last': 136,\n",
       "  'only': 137,\n",
       "  '4': 138,\n",
       "  'down': 139,\n",
       "  'accident': 140,\n",
       "  'war': 141,\n",
       "  \"can't\": 142,\n",
       "  'being': 143,\n",
       "  'gt': 144,\n",
       "  'good': 145,\n",
       "  'make': 146,\n",
       "  'say': 147,\n",
       "  '2015': 148,\n",
       "  'w': 149,\n",
       "  'could': 150,\n",
       "  'may': 151,\n",
       "  'way': 152,\n",
       "  'families': 153,\n",
       "  'many': 154,\n",
       "  'need': 155,\n",
       "  'even': 156,\n",
       "  'years': 157,\n",
       "  'because': 158,\n",
       "  'want': 159,\n",
       "  'best': 160,\n",
       "  'then': 161,\n",
       "  'mass': 162,\n",
       "  'did': 163,\n",
       "  'too': 164,\n",
       "  'collapse': 165,\n",
       "  'right': 166,\n",
       "  'here': 167,\n",
       "  'home': 168,\n",
       "  'bombing': 169,\n",
       "  'him': 170,\n",
       "  'forest': 171,\n",
       "  'army': 172,\n",
       "  'take': 173,\n",
       "  'should': 174,\n",
       "  'work': 175,\n",
       "  'its': 176,\n",
       "  '\\x89û': 177,\n",
       "  'another': 178,\n",
       "  'death': 179,\n",
       "  'really': 180,\n",
       "  'black': 181,\n",
       "  'never': 182,\n",
       "  'please': 183,\n",
       "  'lol': 184,\n",
       "  'hot': 185,\n",
       "  'wildfire': 186,\n",
       "  'look': 187,\n",
       "  'bomber': 188,\n",
       "  'mh370': 189,\n",
       "  'help': 190,\n",
       "  'am': 191,\n",
       "  'god': 192,\n",
       "  'any': 193,\n",
       "  'fatal': 194,\n",
       "  'live': 195,\n",
       "  'northern': 196,\n",
       "  'she': 197,\n",
       "  'let': 198,\n",
       "  \"you're\": 199,\n",
       "  '8': 200,\n",
       "  'pm': 201,\n",
       "  'those': 202,\n",
       "  'obama': 203,\n",
       "  'feel': 204,\n",
       "  'wild': 205,\n",
       "  'much': 206,\n",
       "  '11': 207,\n",
       "  'school': 208,\n",
       "  'flood': 209,\n",
       "  'read': 210,\n",
       "  'where': 211,\n",
       "  'near': 212,\n",
       "  'city': 213,\n",
       "  '9': 214,\n",
       "  'stop': 215,\n",
       "  '6': 216,\n",
       "  'latest': 217,\n",
       "  'before': 218,\n",
       "  'great': 219,\n",
       "  'water': 220,\n",
       "  'said': 221,\n",
       "  'night': 222,\n",
       "  'shit': 223,\n",
       "  'injured': 224,\n",
       "  'top': 225,\n",
       "  'japan': 226,\n",
       "  'services': 227,\n",
       "  'typhoon': 228,\n",
       "  'homes': 229,\n",
       "  'ever': 230,\n",
       "  'during': 231,\n",
       "  'floods': 232,\n",
       "  'under': 233,\n",
       "  'hope': 234,\n",
       "  'im': 235,\n",
       "  'fear': 236,\n",
       "  'atomic': 237,\n",
       "  'come': 238,\n",
       "  '10': 239,\n",
       "  'post': 240,\n",
       "  'truck': 241,\n",
       "  'while': 242,\n",
       "  'house': 243,\n",
       "  'flames': 244,\n",
       "  'ass': 245,\n",
       "  'getting': 246,\n",
       "  '15': 247,\n",
       "  'm': 248,\n",
       "  'wreck': 249,\n",
       "  'every': 250,\n",
       "  'cross': 251,\n",
       "  'damage': 252,\n",
       "  'again': 253,\n",
       "  'change': 254,\n",
       "  'since': 255,\n",
       "  'coming': 256,\n",
       "  'earthquake': 257,\n",
       "  'oil': 258,\n",
       "  'plan': 259,\n",
       "  'red': 260,\n",
       "  '7': 261,\n",
       "  'military': 262,\n",
       "  'severe': 263,\n",
       "  'found': 264,\n",
       "  'these': 265,\n",
       "  'content': 266,\n",
       "  'summer': 267,\n",
       "  'state': 268,\n",
       "  'lightning': 269,\n",
       "  'weather': 270,\n",
       "  'thunderstorm': 271,\n",
       "  'debris': 272,\n",
       "  'family': 273,\n",
       "  'always': 274,\n",
       "  'everyone': 275,\n",
       "  'most': 276,\n",
       "  'next': 277,\n",
       "  'evacuation': 278,\n",
       "  'also': 279,\n",
       "  'natural': 280,\n",
       "  'through': 281,\n",
       "  'heat': 282,\n",
       "  'hit': 283,\n",
       "  'gonna': 284,\n",
       "  'set': 285,\n",
       "  'destroyed': 286,\n",
       "  '\\x89ûò': 287,\n",
       "  'without': 288,\n",
       "  'smoke': 289,\n",
       "  'does': 290,\n",
       "  'photo': 291,\n",
       "  'looks': 292,\n",
       "  \"that's\": 293,\n",
       "  'times': 294,\n",
       "  'wounded': 295,\n",
       "  'devastated': 296,\n",
       "  'well': 297,\n",
       "  'free': 298,\n",
       "  'other': 299,\n",
       "  'check': 300,\n",
       "  'face': 301,\n",
       "  'national': 302,\n",
       "  'cause': 303,\n",
       "  'murder': 304,\n",
       "  'little': 305,\n",
       "  'injuries': 306,\n",
       "  'rain': 307,\n",
       "  'liked': 308,\n",
       "  'trapped': 309,\n",
       "  'terrorist': 310,\n",
       "  'survive': 311,\n",
       "  'spill': 312,\n",
       "  'which': 313,\n",
       "  'fall': 314,\n",
       "  'fucking': 315,\n",
       "  'says': 316,\n",
       "  'service': 317,\n",
       "  '70': 318,\n",
       "  'malaysia': 319,\n",
       "  'run': 320,\n",
       "  'bloody': 321,\n",
       "  'movie': 322,\n",
       "  'story': 323,\n",
       "  'flooding': 324,\n",
       "  'oh': 325,\n",
       "  '08': 326,\n",
       "  'until': 327,\n",
       "  'show': 328,\n",
       "  'hurricane': 329,\n",
       "  'loud': 330,\n",
       "  'around': 331,\n",
       "  'game': 332,\n",
       "  'thunder': 333,\n",
       "  'bad': 334,\n",
       "  'failure': 335,\n",
       "  'saudi': 336,\n",
       "  'o': 337,\n",
       "  'p': 338,\n",
       "  'girl': 339,\n",
       "  'boat': 340,\n",
       "  'put': 341,\n",
       "  'hail': 342,\n",
       "  'area': 343,\n",
       "  'refugees': 344,\n",
       "  'call': 345,\n",
       "  'boy': 346,\n",
       "  'air': 347,\n",
       "  'rescue': 348,\n",
       "  'terrorism': 349,\n",
       "  'survived': 350,\n",
       "  'wind': 351,\n",
       "  'injury': 352,\n",
       "  'someone': 353,\n",
       "  \"i've\": 354,\n",
       "  'drought': 355,\n",
       "  'bag': 356,\n",
       "  'weapon': 357,\n",
       "  'survivors': 358,\n",
       "  'big': 359,\n",
       "  'week': 360,\n",
       "  'released': 361,\n",
       "  'hostage': 362,\n",
       "  'destroy': 363,\n",
       "  'kills': 364,\n",
       "  'landslide': 365,\n",
       "  'report': 366,\n",
       "  'panic': 367,\n",
       "  'screaming': 368,\n",
       "  'weapons': 369,\n",
       "  'breaking': 370,\n",
       "  'warning': 371,\n",
       "  'whole': 372,\n",
       "  'hazard': 373,\n",
       "  'explosion': 374,\n",
       "  'burned': 375,\n",
       "  '30': 376,\n",
       "  'destruction': 377,\n",
       "  'attacked': 378,\n",
       "  'migrants': 379,\n",
       "  'bridge': 380,\n",
       "  'made': 381,\n",
       "  'blood': 382,\n",
       "  'against': 383,\n",
       "  'past': 384,\n",
       "  'missing': 385,\n",
       "  'heard': 386,\n",
       "  'reddit': 387,\n",
       "  'trauma': 388,\n",
       "  'ok': 389,\n",
       "  'women': 390,\n",
       "  'apocalypse': 391,\n",
       "  \"i'll\": 392,\n",
       "  'sinking': 393,\n",
       "  'rescued': 394,\n",
       "  'head': 395,\n",
       "  '40': 396,\n",
       "  'real': 397,\n",
       "  'tonight': 398,\n",
       "  'demolished': 399,\n",
       "  'rescuers': 400,\n",
       "  'hundreds': 401,\n",
       "  'keep': 402,\n",
       "  'sinkhole': 403,\n",
       "  'fuck': 404,\n",
       "  'self': 405,\n",
       "  'catastrophic': 406,\n",
       "  'end': 407,\n",
       "  'drowning': 408,\n",
       "  'mosque': 409,\n",
       "  'high': 410,\n",
       "  'crush': 411,\n",
       "  'responders': 412,\n",
       "  'saw': 413,\n",
       "  'crashed': 414,\n",
       "  'deaths': 415,\n",
       "  'county': 416,\n",
       "  'collapsed': 417,\n",
       "  'evacuate': 418,\n",
       "  'update': 419,\n",
       "  'massacre': 420,\n",
       "  'hazardous': 421,\n",
       "  'blown': 422,\n",
       "  '0': 423,\n",
       "  'thing': 424,\n",
       "  'river': 425,\n",
       "  'explode': 426,\n",
       "  'displaced': 427,\n",
       "  'due': 428,\n",
       "  'white': 429,\n",
       "  'ambulance': 430,\n",
       "  'lava': 431,\n",
       "  'food': 432,\n",
       "  'violent': 433,\n",
       "  'n': 434,\n",
       "  'stock': 435,\n",
       "  'collision': 436,\n",
       "  'riot': 437,\n",
       "  'very': 438,\n",
       "  'screamed': 439,\n",
       "  \"he's\": 440,\n",
       "  'derailment': 441,\n",
       "  'collided': 442,\n",
       "  'away': 443,\n",
       "  'use': 444,\n",
       "  'meltdown': 445,\n",
       "  'bags': 446,\n",
       "  'august': 447,\n",
       "  'traumatised': 448,\n",
       "  'island': 449,\n",
       "  'charged': 450,\n",
       "  'market': 451,\n",
       "  'dust': 452,\n",
       "  'structural': 453,\n",
       "  'wreckage': 454,\n",
       "  'wave': 455,\n",
       "  'lives': 456,\n",
       "  'light': 457,\n",
       "  'least': 458,\n",
       "  'outbreak': 459,\n",
       "  'trouble': 460,\n",
       "  'bombed': 461,\n",
       "  '00': 462,\n",
       "  'drown': 463,\n",
       "  'electrocuted': 464,\n",
       "  'screams': 465,\n",
       "  'ruin': 466,\n",
       "  'battle': 467,\n",
       "  'bang': 468,\n",
       "  'tragedy': 469,\n",
       "  'panicking': 470,\n",
       "  'quarantined': 471,\n",
       "  'curfew': 472,\n",
       "  'catastrophe': 473,\n",
       "  'part': 474,\n",
       "  'wounds': 475,\n",
       "  'crushed': 476,\n",
       "  'blew': 477,\n",
       "  'suspect': 478,\n",
       "  'exploded': 479,\n",
       "  'must': 480,\n",
       "  'inundated': 481,\n",
       "  '05': 482,\n",
       "  'bagging': 483,\n",
       "  'caused': 484,\n",
       "  'deluged': 485,\n",
       "  'bleeding': 486,\n",
       "  'anniversary': 487,\n",
       "  'blast': 488,\n",
       "  'obliterated': 489,\n",
       "  'road': 490,\n",
       "  'investigators': 491,\n",
       "  'danger': 492,\n",
       "  'detonation': 493,\n",
       "  'derailed': 494,\n",
       "  'airplane': 495,\n",
       "  'twister': 496,\n",
       "  'save': 497,\n",
       "  'horrible': 498,\n",
       "  'baby': 499,\n",
       "  'flattened': 500,\n",
       "  'wrecked': 501,\n",
       "  'group': 502,\n",
       "  'obliterate': 503,\n",
       "  'casualties': 504,\n",
       "  'cliff': 505,\n",
       "  'harm': 506,\n",
       "  'arson': 507,\n",
       "  'better': 508,\n",
       "  'engulfed': 509,\n",
       "  'devastation': 510,\n",
       "  'obliteration': 511,\n",
       "  'thought': 512,\n",
       "  'something': 513,\n",
       "  'bus': 514,\n",
       "  'three': 515,\n",
       "  'derail': 516,\n",
       "  'twitter': 517,\n",
       "  'desolation': 518,\n",
       "  'rainstorm': 519,\n",
       "  \"there's\": 520,\n",
       "  'hijacking': 521,\n",
       "  'send': 522,\n",
       "  'seismic': 523,\n",
       "  'calgary': 524,\n",
       "  'chemical': 525,\n",
       "  'sure': 526,\n",
       "  'security': 527,\n",
       "  'half': 528,\n",
       "  'minute': 529,\n",
       "  'cyclone': 530,\n",
       "  'windstorm': 531,\n",
       "  'bioterrorism': 532,\n",
       "  'went': 533,\n",
       "  'quarantine': 534,\n",
       "  'b': 535,\n",
       "  'long': 536,\n",
       "  'power': 537,\n",
       "  'rioting': 538,\n",
       "  'sirens': 539,\n",
       "  'died': 540,\n",
       "  'pkk': 541,\n",
       "  'heart': 542,\n",
       "  'tsunami': 543,\n",
       "  'volcano': 544,\n",
       "  'sandstorm': 545,\n",
       "  'famine': 546,\n",
       "  'hostages': 547,\n",
       "  'whirlwind': 548,\n",
       "  'blazing': 549,\n",
       "  'fedex': 550,\n",
       "  'razed': 551,\n",
       "  'fatality': 552,\n",
       "  'demolish': 553,\n",
       "  'detonated': 554,\n",
       "  'phone': 555,\n",
       "  'collide': 556,\n",
       "  'came': 557,\n",
       "  'tornado': 558,\n",
       "  'hijacker': 559,\n",
       "  'airport': 560,\n",
       "  'sunk': 561,\n",
       "  'things': 562,\n",
       "  'longer': 563,\n",
       "  'rubble': 564,\n",
       "  'line': 565,\n",
       "  'electrocute': 566,\n",
       "  'trying': 567,\n",
       "  'fatalities': 568,\n",
       "  'ebay': 569,\n",
       "  'sound': 570,\n",
       "  'pandemonium': 571,\n",
       "  'st': 572,\n",
       "  'affected': 573,\n",
       "  'snowstorm': 574,\n",
       "  'd': 575,\n",
       "  'evacuated': 576,\n",
       "  'song': 577,\n",
       "  'gets': 578,\n",
       "  'casualty': 579,\n",
       "  'ur': 580,\n",
       "  'drowned': 581,\n",
       "  'fan': 582,\n",
       "  'woman': 583,\n",
       "  'zone': 584,\n",
       "  'eyewitness': 585,\n",
       "  'iran': 586,\n",
       "  'building': 587,\n",
       "  'remember': 588,\n",
       "  'soon': 589,\n",
       "  'turkey': 590,\n",
       "  'south': 591,\n",
       "  'hijack': 592,\n",
       "  'goes': 593,\n",
       "  'used': 594,\n",
       "  'demolition': 595,\n",
       "  'blight': 596,\n",
       "  'murderer': 597,\n",
       "  'government': 598,\n",
       "  'far': 599,\n",
       "  'annihilated': 600,\n",
       "  'stretcher': 601,\n",
       "  'same': 602,\n",
       "  'shoulder': 603,\n",
       "  '\\x89ûó': 604,\n",
       "  'kill': 605,\n",
       "  'e': 606,\n",
       "  'kids': 607,\n",
       "  'mudslide': 608,\n",
       "  \"'the\": 609,\n",
       "  'fight': 610,\n",
       "  'yet': 611,\n",
       "  'land': 612,\n",
       "  'shooting': 613,\n",
       "  'health': 614,\n",
       "  'deluge': 615,\n",
       "  '16yr': 616,\n",
       "  'ablaze': 617,\n",
       "  'tomorrow': 618,\n",
       "  'stand': 619,\n",
       "  'detonate': 620,\n",
       "  'low': 621,\n",
       "  'wanna': 622,\n",
       "  'plane': 623,\n",
       "  'music': 624,\n",
       "  'bioterror': 625,\n",
       "  'officer': 626,\n",
       "  'play': 627,\n",
       "  'though': 628,\n",
       "  'india': 629,\n",
       "  'tell': 630,\n",
       "  'days': 631,\n",
       "  'cool': 632,\n",
       "  'issues': 633,\n",
       "  'nothing': 634,\n",
       "  'order': 635,\n",
       "  'possible': 636,\n",
       "  'prebreak': 637,\n",
       "  'such': 638,\n",
       "  'actually': 639,\n",
       "  'thanks': 640,\n",
       "  \"legionnaires'\": 641,\n",
       "  'annihilation': 642,\n",
       "  'start': 643,\n",
       "  'hellfire': 644,\n",
       "  'believe': 645,\n",
       "  'pic': 646,\n",
       "  'lot': 647,\n",
       "  'c': 648,\n",
       "  'isis': 649,\n",
       "  'done': 650,\n",
       "  \"doesn't\": 651,\n",
       "  'yes': 652,\n",
       "  'west': 653,\n",
       "  'left': 654,\n",
       "  'care': 655,\n",
       "  'doing': 656,\n",
       "  'armageddon': 657,\n",
       "  'officials': 658,\n",
       "  'men': 659,\n",
       "  'already': 660,\n",
       "  'r': 661,\n",
       "  'desolate': 662,\n",
       "  'upheaval': 663,\n",
       "  'stay': 664,\n",
       "  '16': 665,\n",
       "  'brown': 666,\n",
       "  'caught': 667,\n",
       "  'reunion': 668,\n",
       "  'israeli': 669,\n",
       "  'media': 670,\n",
       "  'avalanche': 671,\n",
       "  'person': 672,\n",
       "  'site': 673,\n",
       "  'aug': 674,\n",
       "  'abc': 675,\n",
       "  'hell': 676,\n",
       "  'fun': 677,\n",
       "  'nearby': 678,\n",
       "  'hailstorm': 679,\n",
       "  'die': 680,\n",
       "  'park': 681,\n",
       "  'thank': 682,\n",
       "  'ago': 683,\n",
       "  'bush': 684,\n",
       "  'mp': 685,\n",
       "  'damn': 686,\n",
       "  'horror': 687,\n",
       "  'case': 688,\n",
       "  'north': 689,\n",
       "  'traffic': 690,\n",
       "  'deal': 691,\n",
       "  're\\x89û': 692,\n",
       "  'try': 693,\n",
       "  'reactor': 694,\n",
       "  'hours': 695,\n",
       "  'wait': 696,\n",
       "  'making': 697,\n",
       "  'wake': 698,\n",
       "  'rise': 699,\n",
       "  'mayhem': 700,\n",
       "  'hear': 701,\n",
       "  'support': 702,\n",
       "  'siren': 703,\n",
       "  'own': 704,\n",
       "  'pretty': 705,\n",
       "  'history': 706,\n",
       "  'street': 707,\n",
       "  'data': 708,\n",
       "  'hat': 709,\n",
       "  'plans': 710,\n",
       "  'few': 711,\n",
       "  'policy': 712,\n",
       "  '01': 713,\n",
       "  'takes': 714,\n",
       "  'swallowed': 715,\n",
       "  'american': 716,\n",
       "  'watching': 717,\n",
       "  'declares': 718,\n",
       "  'israel': 719,\n",
       "  'wow': 720,\n",
       "  'aftershock': 721,\n",
       "  'children': 722,\n",
       "  'yeah': 723,\n",
       "  'find': 724,\n",
       "  'inside': 725,\n",
       "  'lt': 726,\n",
       "  'pick': 727,\n",
       "  'sue': 728,\n",
       "  'legionnaires': 729,\n",
       "  \"didn't\": 730,\n",
       "  'blizzard': 731,\n",
       "  \"i'd\": 732,\n",
       "  'eyes': 733,\n",
       "  'tv': 734,\n",
       "  'saipan': 735,\n",
       "  'having': 736,\n",
       "  'almost': 737,\n",
       "  'reuters': 738,\n",
       "  'emmerdale': 739,\n",
       "  'listen': 740,\n",
       "  'happy': 741,\n",
       "  'waves': 742,\n",
       "  'fukushima': 743,\n",
       "  'words': 744,\n",
       "  'trench': 745,\n",
       "  'anyone': 746,\n",
       "  'shot': 747,\n",
       "  'might': 748,\n",
       "  'lab': 749,\n",
       "  'give': 750,\n",
       "  'money': 751,\n",
       "  'effect': 752,\n",
       "  '06': 753,\n",
       "  'peace': 754,\n",
       "  'crazy': 755,\n",
       "  'makes': 756,\n",
       "  'feared': 757,\n",
       "  'move': 758,\n",
       "  '60': 759,\n",
       "  'guy': 760,\n",
       "  'omg': 761,\n",
       "  'bigger': 762,\n",
       "  'anything': 763,\n",
       "  'x': 764,\n",
       "  'finally': 765,\n",
       "  \"'conclusively\": 766,\n",
       "  \"confirmed'\": 767,\n",
       "  'bar': 768,\n",
       "  \"'i\": 769,\n",
       "  'guys': 770,\n",
       "  '13': 771,\n",
       "  '50': 772,\n",
       "  'probably': 773,\n",
       "  'second': 774,\n",
       "  'business': 775,\n",
       "  'myself': 776,\n",
       "  'arsonist': 777,\n",
       "  'literally': 778,\n",
       "  'houses': 779,\n",
       "  'photos': 780,\n",
       "  'morning': 781,\n",
       "  'crisis': 782,\n",
       "  'transport': 783,\n",
       "  \"what's\": 784,\n",
       "  'seen': 785,\n",
       "  '25': 786,\n",
       "  'maybe': 787,\n",
       "  'helicopter': 788,\n",
       "  'bc': 789,\n",
       "  'searching': 790,\n",
       "  'bbc': 791,\n",
       "  'happened': 792,\n",
       "  'team': 793,\n",
       "  'book': 794,\n",
       "  'saved': 795,\n",
       "  'dont': 796,\n",
       "  'job': 797,\n",
       "  'property': 798,\n",
       "  'sorry': 799,\n",
       "  \"they're\": 800,\n",
       "  'yours': 801,\n",
       "  'beautiful': 802,\n",
       "  'soudelor': 803,\n",
       "  'place': 804,\n",
       "  'outside': 805,\n",
       "  'mom': 806,\n",
       "  'called': 807,\n",
       "  \"isn't\": 808,\n",
       "  'memories': 809,\n",
       "  'bestnaijamade': 810,\n",
       "  'public': 811,\n",
       "  'hate': 812,\n",
       "  'side': 813,\n",
       "  '100': 814,\n",
       "  'everything': 815,\n",
       "  'both': 816,\n",
       "  'huge': 817,\n",
       "  'yourself': 818,\n",
       "  'leave': 819,\n",
       "  'lost': 820,\n",
       "  'muslims': 821,\n",
       "  'knock': 822,\n",
       "  'sensor': 823,\n",
       "  'likely': 824,\n",
       "  'hey': 825,\n",
       "  'bodies': 826,\n",
       "  'name': 827,\n",
       "  'blaze': 828,\n",
       "  'room': 829,\n",
       "  \"we're\": 830,\n",
       "  'hollywood': 831,\n",
       "  'salt': 832,\n",
       "  'truth': 833,\n",
       "  'businesses': 834,\n",
       "  'crews': 835,\n",
       "  'nowplaying': 836,\n",
       "  'islam': 837,\n",
       "  'spot': 838,\n",
       "  'amid': 839,\n",
       "  'pakistan': 840,\n",
       "  'center': 841,\n",
       "  'follow': 842,\n",
       "  'taken': 843,\n",
       "  'signs': 844,\n",
       "  'marks': 845,\n",
       "  'pray': 846,\n",
       "  'manslaughter': 847,\n",
       "  'ca': 848,\n",
       "  'control': 849,\n",
       "  'aircraft': 850,\n",
       "  'level': 851,\n",
       "  'computers': 852,\n",
       "  'led': 853,\n",
       "  'link': 854,\n",
       "  'major': 855,\n",
       "  'giant': 856,\n",
       "  'drake': 857,\n",
       "  'green': 858,\n",
       "  'quiz': 859,\n",
       "  'flag': 860,\n",
       "  'reason': 861,\n",
       "  'australia': 862,\n",
       "  'town': 863,\n",
       "  'offensive': 864,\n",
       "  'ball': 865,\n",
       "  'projected': 866,\n",
       "  '20': 867,\n",
       "  'talk': 868,\n",
       "  'anthrax': 869,\n",
       "  'rd': 870,\n",
       "  'course': 871,\n",
       "  'america': 872,\n",
       "  'gems': 873,\n",
       "  'playing': 874,\n",
       "  'running': 875,\n",
       "  'appears': 876,\n",
       "  'bring': 877,\n",
       "  'mad': 878,\n",
       "  'trains': 879,\n",
       "  '12': 880,\n",
       "  'leather': 881,\n",
       "  'feeling': 882,\n",
       "  'horse': 883,\n",
       "  'banned': 884,\n",
       "  'become': 885,\n",
       "  'texas': 886,\n",
       "  'image': 887,\n",
       "  'russian': 888,\n",
       "  'fast': 889,\n",
       "  'ignition': 890,\n",
       "  'nearly': 891,\n",
       "  'miss': 892,\n",
       "  'wrong': 893,\n",
       "  '70th': 894,\n",
       "  'flight': 895,\n",
       "  'virgin': 896,\n",
       "  'flash': 897,\n",
       "  \"let's\": 898,\n",
       "  'eye': 899,\n",
       "  'issued': 900,\n",
       "  'ancient': 901,\n",
       "  'comes': 902,\n",
       "  'dog': 903,\n",
       "  'safety': 904,\n",
       "  'country': 905,\n",
       "  'lord': 906,\n",
       "  '17': 907,\n",
       "  'child': 908,\n",
       "  'reports': 909,\n",
       "  'confirmed': 910,\n",
       "  'needs': 911,\n",
       "  'shift': 912,\n",
       "  'christian': 913,\n",
       "  'cnn': 914,\n",
       "  'refugio': 915,\n",
       "  'costlier': 916,\n",
       "  'chicago': 917,\n",
       "  'across': 918,\n",
       "  'radio': 919,\n",
       "  'space': 920,\n",
       "  'action': 921,\n",
       "  'else': 922,\n",
       "  'closed': 923,\n",
       "  'worst': 924,\n",
       "  'vehicle': 925,\n",
       "  'climate': 926,\n",
       "  'picking': 927,\n",
       "  'east': 928,\n",
       "  'large': 929,\n",
       "  'entire': 930,\n",
       "  'driving': 931,\n",
       "  'favorite': 932,\n",
       "  'la': 933,\n",
       "  'ladies': 934,\n",
       "  'king': 935,\n",
       "  'funtenna': 936,\n",
       "  'miners': 937,\n",
       "  'village': 938,\n",
       "  'temple': 939,\n",
       "  'okay': 940,\n",
       "  'cars': 941,\n",
       "  'militants': 942,\n",
       "  'hard': 943,\n",
       "  'sign': 944,\n",
       "  'told': 945,\n",
       "  'insurance': 946,\n",
       "  'once': 947,\n",
       "  'mount': 948,\n",
       "  'local': 949,\n",
       "  'super': 950,\n",
       "  'class': 951,\n",
       "  'haha': 952,\n",
       "  'heavy': 953,\n",
       "  'mishaps': 954,\n",
       "  'thursday': 955,\n",
       "  'online': 956,\n",
       "  'sad': 957,\n",
       "  'galactic': 958,\n",
       "  'claims': 959,\n",
       "  'uk': 960,\n",
       "  'unconfirmed': 961,\n",
       "  \"neighbour's\": 962,\n",
       "  'germs': 963,\n",
       "  'open': 964,\n",
       "  'learn': 965,\n",
       "  'united': 966,\n",
       "  'human': 967,\n",
       "  'others': 968,\n",
       "  'youth': 969,\n",
       "  'meek': 970,\n",
       "  'enough': 971,\n",
       "  'four': 972,\n",
       "  'true': 973,\n",
       "  'disea': 974,\n",
       "  'moment': 975,\n",
       "  'hand': 976,\n",
       "  'friends': 977,\n",
       "  'alarm': 978,\n",
       "  '18': 979,\n",
       "  'official': 980,\n",
       "  'win': 981,\n",
       "  'mph': 982,\n",
       "  'daily': 983,\n",
       "  'piece': 984,\n",
       "  'rock': 985,\n",
       "  'china': 986,\n",
       "  \"ain't\": 987,\n",
       "  'declaration': 988,\n",
       "  \"china's\": 989,\n",
       "  'front': 990,\n",
       "  'buy': 991,\n",
       "  \"won't\": 992,\n",
       "  \"'we're\": 993,\n",
       "  'firefighters': 994,\n",
       "  'break': 995,\n",
       "  'earth': 996,\n",
       "  'added': 997,\n",
       "  \"reddit's\": 998,\n",
       "  'subreddits': 999,\n",
       "  'wonder': 1000,\n",
       "  ...})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_pad,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(model)\n",
    "model['king'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 29319/29319 [00:03<00:00, 7723.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/100\n",
      "6851/6851 [==============================] - ETA: 29s - loss: 0.70 - ETA: 22s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 16s - loss: 0.68 - ETA: 14s - loss: 0.67 - ETA: 12s - loss: 0.67 - ETA: 10s - loss: 0.67 - ETA: 8s - loss: 0.6753 - ETA: 7s - loss: 0.672 - ETA: 5s - loss: 0.669 - ETA: 3s - loss: 0.666 - ETA: 2s - loss: 0.665 - ETA: 0s - loss: 0.664 - 22s 3ms/step - loss: 0.6630 - val_loss: 0.6100\n",
      "Epoch 2/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.65 - ETA: 16s - loss: 0.65 - ETA: 15s - loss: 0.64 - ETA: 13s - loss: 0.63 - ETA: 12s - loss: 0.62 - ETA: 10s - loss: 0.61 - ETA: 9s - loss: 0.6139 - ETA: 7s - loss: 0.609 - ETA: 6s - loss: 0.604 - ETA: 5s - loss: 0.601 - ETA: 3s - loss: 0.597 - ETA: 2s - loss: 0.594 - ETA: 0s - loss: 0.590 - 21s 3ms/step - loss: 0.5894 - val_loss: 0.4622\n",
      "Epoch 3/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.59 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 12s - loss: 0.55 - ETA: 11s - loss: 0.54 - ETA: 9s - loss: 0.5382 - ETA: 8s - loss: 0.537 - ETA: 6s - loss: 0.532 - ETA: 5s - loss: 0.531 - ETA: 3s - loss: 0.532 - ETA: 2s - loss: 0.532 - ETA: 0s - loss: 0.530 - 20s 3ms/step - loss: 0.5299 - val_loss: 0.4358\n",
      "Epoch 4/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.53 - ETA: 16s - loss: 0.50 - ETA: 15s - loss: 0.51 - ETA: 13s - loss: 0.51 - ETA: 12s - loss: 0.51 - ETA: 10s - loss: 0.50 - ETA: 8s - loss: 0.5099 - ETA: 7s - loss: 0.508 - ETA: 6s - loss: 0.507 - ETA: 4s - loss: 0.505 - ETA: 3s - loss: 0.505 - ETA: 1s - loss: 0.502 - ETA: 0s - loss: 0.500 - 20s 3ms/step - loss: 0.5006 - val_loss: 0.4307\n",
      "Epoch 5/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.45 - ETA: 16s - loss: 0.47 - ETA: 15s - loss: 0.47 - ETA: 13s - loss: 0.48 - ETA: 12s - loss: 0.49 - ETA: 10s - loss: 0.49 - ETA: 9s - loss: 0.5007 - ETA: 7s - loss: 0.495 - ETA: 6s - loss: 0.493 - ETA: 5s - loss: 0.488 - ETA: 3s - loss: 0.494 - ETA: 2s - loss: 0.492 - ETA: 0s - loss: 0.488 - 21s 3ms/step - loss: 0.4897 - val_loss: 0.4330\n",
      "Epoch 6/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.49 - ETA: 15s - loss: 0.47 - ETA: 14s - loss: 0.48 - ETA: 13s - loss: 0.48 - ETA: 12s - loss: 0.48 - ETA: 11s - loss: 0.48 - ETA: 9s - loss: 0.4801 - ETA: 7s - loss: 0.475 - ETA: 6s - loss: 0.476 - ETA: 4s - loss: 0.478 - ETA: 3s - loss: 0.480 - ETA: 2s - loss: 0.479 - ETA: 0s - loss: 0.479 - 21s 3ms/step - loss: 0.4814 - val_loss: 0.4261\n",
      "Epoch 7/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.46 - ETA: 16s - loss: 0.48 - ETA: 15s - loss: 0.48 - ETA: 13s - loss: 0.49 - ETA: 12s - loss: 0.49 - ETA: 10s - loss: 0.48 - ETA: 9s - loss: 0.4761 - ETA: 7s - loss: 0.471 - ETA: 6s - loss: 0.474 - ETA: 5s - loss: 0.477 - ETA: 3s - loss: 0.482 - ETA: 2s - loss: 0.481 - ETA: 0s - loss: 0.479 - 21s 3ms/step - loss: 0.4813 - val_loss: 0.4271\n",
      "Epoch 8/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.45 - ETA: 18s - loss: 0.45 - ETA: 16s - loss: 0.46 - ETA: 14s - loss: 0.46 - ETA: 12s - loss: 0.46 - ETA: 11s - loss: 0.46 - ETA: 9s - loss: 0.4672 - ETA: 8s - loss: 0.467 - ETA: 6s - loss: 0.471 - ETA: 5s - loss: 0.473 - ETA: 3s - loss: 0.474 - ETA: 2s - loss: 0.476 - ETA: 0s - loss: 0.479 - 22s 3ms/step - loss: 0.4770 - val_loss: 0.4144\n",
      "Epoch 9/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.47 - ETA: 16s - loss: 0.46 - ETA: 14s - loss: 0.45 - ETA: 13s - loss: 0.45 - ETA: 11s - loss: 0.44 - ETA: 10s - loss: 0.45 - ETA: 9s - loss: 0.4624 - ETA: 7s - loss: 0.460 - ETA: 6s - loss: 0.460 - ETA: 4s - loss: 0.462 - ETA: 3s - loss: 0.464 - ETA: 2s - loss: 0.466 - ETA: 0s - loss: 0.467 - 21s 3ms/step - loss: 0.4661 - val_loss: 0.4215\n",
      "Epoch 10/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.46 - ETA: 18s - loss: 0.44 - ETA: 16s - loss: 0.45 - ETA: 14s - loss: 0.44 - ETA: 12s - loss: 0.45 - ETA: 11s - loss: 0.45 - ETA: 9s - loss: 0.4557 - ETA: 8s - loss: 0.455 - ETA: 6s - loss: 0.455 - ETA: 5s - loss: 0.456 - ETA: 3s - loss: 0.456 - ETA: 2s - loss: 0.456 - ETA: 0s - loss: 0.457 - 22s 3ms/step - loss: 0.4575 - val_loss: 0.4159\n",
      "Epoch 11/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.43 - ETA: 17s - loss: 0.46 - ETA: 15s - loss: 0.45 - ETA: 14s - loss: 0.46 - ETA: 13s - loss: 0.47 - ETA: 11s - loss: 0.47 - ETA: 9s - loss: 0.4753 - ETA: 8s - loss: 0.478 - ETA: 6s - loss: 0.476 - ETA: 5s - loss: 0.473 - ETA: 3s - loss: 0.470 - ETA: 2s - loss: 0.472 - ETA: 0s - loss: 0.474 - 22s 3ms/step - loss: 0.4717 - val_loss: 0.4137\n",
      "Epoch 12/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.47 - ETA: 18s - loss: 0.44 - ETA: 15s - loss: 0.45 - ETA: 14s - loss: 0.45 - ETA: 12s - loss: 0.45 - ETA: 11s - loss: 0.45 - ETA: 9s - loss: 0.4569 - ETA: 8s - loss: 0.455 - ETA: 6s - loss: 0.459 - ETA: 5s - loss: 0.465 - ETA: 3s - loss: 0.465 - ETA: 2s - loss: 0.466 - ETA: 0s - loss: 0.466 - 21s 3ms/step - loss: 0.4679 - val_loss: 0.4260\n",
      "Epoch 13/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.45 - ETA: 16s - loss: 0.46 - ETA: 14s - loss: 0.46 - ETA: 13s - loss: 0.45 - ETA: 12s - loss: 0.46 - ETA: 10s - loss: 0.46 - ETA: 9s - loss: 0.4646 - ETA: 7s - loss: 0.464 - ETA: 6s - loss: 0.462 - ETA: 4s - loss: 0.459 - ETA: 3s - loss: 0.456 - ETA: 2s - loss: 0.458 - ETA: 0s - loss: 0.460 - 21s 3ms/step - loss: 0.4593 - val_loss: 0.4176\n",
      "Epoch 14/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.44 - ETA: 17s - loss: 0.43 - ETA: 16s - loss: 0.46 - ETA: 14s - loss: 0.45 - ETA: 12s - loss: 0.44 - ETA: 11s - loss: 0.44 - ETA: 9s - loss: 0.4514 - ETA: 8s - loss: 0.453 - ETA: 6s - loss: 0.451 - ETA: 5s - loss: 0.451 - ETA: 3s - loss: 0.456 - ETA: 2s - loss: 0.452 - ETA: 0s - loss: 0.455 - 22s 3ms/step - loss: 0.4553 - val_loss: 0.4128\n",
      "Epoch 15/100\n",
      "6851/6851 [==============================] - ETA: 21s - loss: 0.42 - ETA: 18s - loss: 0.43 - ETA: 17s - loss: 0.42 - ETA: 15s - loss: 0.43 - ETA: 14s - loss: 0.43 - ETA: 12s - loss: 0.44 - ETA: 10s - loss: 0.44 - ETA: 8s - loss: 0.4466 - ETA: 6s - loss: 0.447 - ETA: 5s - loss: 0.447 - ETA: 3s - loss: 0.448 - ETA: 2s - loss: 0.446 - ETA: 0s - loss: 0.447 - 22s 3ms/step - loss: 0.4485 - val_loss: 0.4116\n",
      "Epoch 16/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.44 - ETA: 16s - loss: 0.46 - ETA: 14s - loss: 0.46 - ETA: 13s - loss: 0.45 - ETA: 12s - loss: 0.44 - ETA: 10s - loss: 0.43 - ETA: 9s - loss: 0.4414 - ETA: 7s - loss: 0.449 - ETA: 6s - loss: 0.450 - ETA: 4s - loss: 0.450 - ETA: 3s - loss: 0.452 - ETA: 1s - loss: 0.450 - ETA: 0s - loss: 0.450 - 20s 3ms/step - loss: 0.4516 - val_loss: 0.4090\n",
      "Epoch 17/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.43 - ETA: 16s - loss: 0.44 - ETA: 14s - loss: 0.43 - ETA: 13s - loss: 0.43 - ETA: 12s - loss: 0.42 - ETA: 10s - loss: 0.43 - ETA: 9s - loss: 0.4416 - ETA: 8s - loss: 0.449 - ETA: 6s - loss: 0.448 - ETA: 4s - loss: 0.450 - ETA: 3s - loss: 0.449 - ETA: 2s - loss: 0.447 - ETA: 0s - loss: 0.446 - 21s 3ms/step - loss: 0.4460 - val_loss: 0.4069\n",
      "Epoch 18/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.44 - ETA: 17s - loss: 0.45 - ETA: 16s - loss: 0.43 - ETA: 14s - loss: 0.44 - ETA: 13s - loss: 0.44 - ETA: 11s - loss: 0.44 - ETA: 9s - loss: 0.4472 - ETA: 8s - loss: 0.445 - ETA: 6s - loss: 0.444 - ETA: 5s - loss: 0.445 - ETA: 3s - loss: 0.445 - ETA: 2s - loss: 0.443 - ETA: 0s - loss: 0.443 - 21s 3ms/step - loss: 0.4464 - val_loss: 0.4215\n",
      "Epoch 19/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.41 - ETA: 17s - loss: 0.43 - ETA: 16s - loss: 0.43 - ETA: 14s - loss: 0.43 - ETA: 12s - loss: 0.44 - ETA: 11s - loss: 0.44 - ETA: 10s - loss: 0.43 - ETA: 8s - loss: 0.4369 - ETA: 6s - loss: 0.437 - ETA: 5s - loss: 0.439 - ETA: 3s - loss: 0.437 - ETA: 2s - loss: 0.441 - ETA: 0s - loss: 0.439 - 22s 3ms/step - loss: 0.4402 - val_loss: 0.4124\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6851/6851 [==============================] - ETA: 17s - loss: 0.43 - ETA: 16s - loss: 0.42 - ETA: 15s - loss: 0.43 - ETA: 14s - loss: 0.43 - ETA: 12s - loss: 0.42 - ETA: 11s - loss: 0.42 - ETA: 9s - loss: 0.4223 - ETA: 8s - loss: 0.425 - ETA: 6s - loss: 0.427 - ETA: 5s - loss: 0.429 - ETA: 3s - loss: 0.427 - ETA: 2s - loss: 0.433 - ETA: 0s - loss: 0.434 - 22s 3ms/step - loss: 0.4347 - val_loss: 0.4172\n",
      "Epoch 21/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.46 - ETA: 18s - loss: 0.44 - ETA: 16s - loss: 0.45 - ETA: 14s - loss: 0.45 - ETA: 13s - loss: 0.44 - ETA: 11s - loss: 0.44 - ETA: 10s - loss: 0.44 - ETA: 8s - loss: 0.4436 - ETA: 6s - loss: 0.439 - ETA: 5s - loss: 0.439 - ETA: 3s - loss: 0.439 - ETA: 2s - loss: 0.439 - ETA: 0s - loss: 0.439 - 22s 3ms/step - loss: 0.4409 - val_loss: 0.4058\n",
      "Epoch 22/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.43 - ETA: 16s - loss: 0.42 - ETA: 14s - loss: 0.43 - ETA: 14s - loss: 0.44 - ETA: 12s - loss: 0.44 - ETA: 11s - loss: 0.44 - ETA: 9s - loss: 0.4389 - ETA: 7s - loss: 0.440 - ETA: 6s - loss: 0.438 - ETA: 5s - loss: 0.436 - ETA: 3s - loss: 0.439 - ETA: 2s - loss: 0.440 - ETA: 0s - loss: 0.438 - 21s 3ms/step - loss: 0.4368 - val_loss: 0.4069\n",
      "Epoch 23/100\n",
      "6851/6851 [==============================] - ETA: 21s - loss: 0.46 - ETA: 20s - loss: 0.46 - ETA: 18s - loss: 0.44 - ETA: 16s - loss: 0.43 - ETA: 14s - loss: 0.42 - ETA: 12s - loss: 0.42 - ETA: 10s - loss: 0.41 - ETA: 8s - loss: 0.4192 - ETA: 7s - loss: 0.417 - ETA: 5s - loss: 0.418 - ETA: 3s - loss: 0.419 - ETA: 2s - loss: 0.419 - ETA: 0s - loss: 0.422 - 23s 3ms/step - loss: 0.4215 - val_loss: 0.4022\n",
      "Epoch 24/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.42 - ETA: 17s - loss: 0.44 - ETA: 16s - loss: 0.44 - ETA: 14s - loss: 0.43 - ETA: 13s - loss: 0.43 - ETA: 11s - loss: 0.43 - ETA: 10s - loss: 0.43 - ETA: 8s - loss: 0.4240 - ETA: 6s - loss: 0.424 - ETA: 5s - loss: 0.419 - ETA: 3s - loss: 0.425 - ETA: 2s - loss: 0.424 - ETA: 0s - loss: 0.427 - 22s 3ms/step - loss: 0.4288 - val_loss: 0.4064\n",
      "Epoch 25/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.41 - ETA: 18s - loss: 0.41 - ETA: 16s - loss: 0.40 - ETA: 15s - loss: 0.41 - ETA: 13s - loss: 0.41 - ETA: 11s - loss: 0.42 - ETA: 10s - loss: 0.42 - ETA: 8s - loss: 0.4179 - ETA: 7s - loss: 0.422 - ETA: 5s - loss: 0.425 - ETA: 3s - loss: 0.424 - ETA: 2s - loss: 0.424 - ETA: 0s - loss: 0.427 - 23s 3ms/step - loss: 0.4279 - val_loss: 0.4076\n",
      "Epoch 26/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.38 - ETA: 18s - loss: 0.39 - ETA: 16s - loss: 0.41 - ETA: 14s - loss: 0.41 - ETA: 13s - loss: 0.41 - ETA: 11s - loss: 0.41 - ETA: 10s - loss: 0.41 - ETA: 8s - loss: 0.4192 - ETA: 7s - loss: 0.421 - ETA: 5s - loss: 0.427 - ETA: 3s - loss: 0.425 - ETA: 2s - loss: 0.425 - ETA: 0s - loss: 0.424 - 21s 3ms/step - loss: 0.4252 - val_loss: 0.4074\n",
      "Epoch 27/100\n",
      "6851/6851 [==============================] - ETA: 14s - loss: 0.43 - ETA: 13s - loss: 0.41 - ETA: 12s - loss: 0.41 - ETA: 11s - loss: 0.41 - ETA: 10s - loss: 0.41 - ETA: 9s - loss: 0.4200 - ETA: 7s - loss: 0.421 - ETA: 6s - loss: 0.422 - ETA: 5s - loss: 0.431 - ETA: 4s - loss: 0.429 - ETA: 2s - loss: 0.430 - ETA: 1s - loss: 0.428 - ETA: 0s - loss: 0.426 - 17s 3ms/step - loss: 0.4264 - val_loss: 0.4054\n",
      "Epoch 28/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.44 - ETA: 15s - loss: 0.42 - ETA: 14s - loss: 0.42 - ETA: 13s - loss: 0.42 - ETA: 11s - loss: 0.41 - ETA: 10s - loss: 0.42 - ETA: 9s - loss: 0.4239 - ETA: 7s - loss: 0.428 - ETA: 6s - loss: 0.429 - ETA: 4s - loss: 0.433 - ETA: 3s - loss: 0.429 - ETA: 1s - loss: 0.423 - ETA: 0s - loss: 0.424 - 20s 3ms/step - loss: 0.4229 - val_loss: 0.4052\n",
      "Epoch 29/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.42 - ETA: 16s - loss: 0.42 - ETA: 16s - loss: 0.42 - ETA: 14s - loss: 0.41 - ETA: 13s - loss: 0.41 - ETA: 11s - loss: 0.41 - ETA: 9s - loss: 0.4144 - ETA: 8s - loss: 0.418 - ETA: 6s - loss: 0.421 - ETA: 5s - loss: 0.422 - ETA: 3s - loss: 0.423 - ETA: 2s - loss: 0.425 - ETA: 0s - loss: 0.425 - 21s 3ms/step - loss: 0.4235 - val_loss: 0.4037\n",
      "Epoch 30/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.40 - ETA: 16s - loss: 0.42 - ETA: 14s - loss: 0.41 - ETA: 13s - loss: 0.41 - ETA: 11s - loss: 0.42 - ETA: 10s - loss: 0.42 - ETA: 8s - loss: 0.4206 - ETA: 7s - loss: 0.427 - ETA: 6s - loss: 0.428 - ETA: 4s - loss: 0.430 - ETA: 3s - loss: 0.426 - ETA: 1s - loss: 0.424 - ETA: 0s - loss: 0.424 - 20s 3ms/step - loss: 0.4224 - val_loss: 0.4081\n",
      "Epoch 31/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.39 - ETA: 14s - loss: 0.42 - ETA: 13s - loss: 0.42 - ETA: 12s - loss: 0.41 - ETA: 11s - loss: 0.41 - ETA: 10s - loss: 0.41 - ETA: 8s - loss: 0.4157 - ETA: 7s - loss: 0.412 - ETA: 5s - loss: 0.411 - ETA: 4s - loss: 0.412 - ETA: 3s - loss: 0.417 - ETA: 1s - loss: 0.415 - ETA: 0s - loss: 0.413 - 19s 3ms/step - loss: 0.4147 - val_loss: 0.4098\n",
      "Epoch 32/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.36 - ETA: 17s - loss: 0.38 - ETA: 15s - loss: 0.38 - ETA: 13s - loss: 0.38 - ETA: 12s - loss: 0.39 - ETA: 10s - loss: 0.39 - ETA: 9s - loss: 0.4001 - ETA: 7s - loss: 0.405 - ETA: 6s - loss: 0.406 - ETA: 4s - loss: 0.406 - ETA: 3s - loss: 0.406 - ETA: 1s - loss: 0.405 - ETA: 0s - loss: 0.406 - 20s 3ms/step - loss: 0.4057 - val_loss: 0.4129\n",
      "Epoch 33/100\n",
      "6851/6851 [==============================] - ETA: 24s - loss: 0.39 - ETA: 21s - loss: 0.43 - ETA: 19s - loss: 0.42 - ETA: 16s - loss: 0.42 - ETA: 15s - loss: 0.40 - ETA: 13s - loss: 0.41 - ETA: 11s - loss: 0.41 - ETA: 9s - loss: 0.4164 - ETA: 7s - loss: 0.415 - ETA: 5s - loss: 0.413 - ETA: 3s - loss: 0.409 - ETA: 2s - loss: 0.407 - ETA: 0s - loss: 0.404 - 23s 3ms/step - loss: 0.4054 - val_loss: 0.4070\n",
      "Epoch 34/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.36 - ETA: 15s - loss: 0.37 - ETA: 14s - loss: 0.38 - ETA: 12s - loss: 0.39 - ETA: 11s - loss: 0.39 - ETA: 10s - loss: 0.40 - ETA: 8s - loss: 0.3997 - ETA: 7s - loss: 0.394 - ETA: 6s - loss: 0.398 - ETA: 4s - loss: 0.398 - ETA: 3s - loss: 0.396 - ETA: 1s - loss: 0.397 - ETA: 0s - loss: 0.401 - 19s 3ms/step - loss: 0.4045 - val_loss: 0.4052\n",
      "Epoch 35/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.43 - ETA: 15s - loss: 0.41 - ETA: 13s - loss: 0.40 - ETA: 12s - loss: 0.39 - ETA: 11s - loss: 0.40 - ETA: 10s - loss: 0.40 - ETA: 8s - loss: 0.4000 - ETA: 7s - loss: 0.398 - ETA: 6s - loss: 0.400 - ETA: 4s - loss: 0.405 - ETA: 3s - loss: 0.404 - ETA: 1s - loss: 0.403 - ETA: 0s - loss: 0.401 - 19s 3ms/step - loss: 0.4020 - val_loss: 0.4064\n",
      "Epoch 36/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.39 - ETA: 16s - loss: 0.40 - ETA: 14s - loss: 0.40 - ETA: 13s - loss: 0.39 - ETA: 11s - loss: 0.39 - ETA: 10s - loss: 0.39 - ETA: 9s - loss: 0.3947 - ETA: 7s - loss: 0.397 - ETA: 6s - loss: 0.394 - ETA: 4s - loss: 0.393 - ETA: 3s - loss: 0.395 - ETA: 1s - loss: 0.395 - ETA: 0s - loss: 0.396 - 20s 3ms/step - loss: 0.3957 - val_loss: 0.4155\n",
      "Epoch 37/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.39 - ETA: 15s - loss: 0.37 - ETA: 14s - loss: 0.37 - ETA: 12s - loss: 0.37 - ETA: 11s - loss: 0.38 - ETA: 10s - loss: 0.38 - ETA: 8s - loss: 0.3898 - ETA: 7s - loss: 0.392 - ETA: 5s - loss: 0.391 - ETA: 4s - loss: 0.394 - ETA: 3s - loss: 0.389 - ETA: 1s - loss: 0.392 - ETA: 0s - loss: 0.394 - 19s 3ms/step - loss: 0.3953 - val_loss: 0.4108\n",
      "Epoch 38/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.42 - ETA: 16s - loss: 0.40 - ETA: 15s - loss: 0.41 - ETA: 13s - loss: 0.41 - ETA: 12s - loss: 0.40 - ETA: 10s - loss: 0.40 - ETA: 9s - loss: 0.4016 - ETA: 7s - loss: 0.400 - ETA: 6s - loss: 0.399 - ETA: 4s - loss: 0.398 - ETA: 3s - loss: 0.402 - ETA: 1s - loss: 0.401 - ETA: 0s - loss: 0.401 - 20s 3ms/step - loss: 0.4017 - val_loss: 0.4082\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6851/6851 [==============================] - ETA: 18s - loss: 0.35 - ETA: 17s - loss: 0.37 - ETA: 15s - loss: 0.37 - ETA: 13s - loss: 0.38 - ETA: 11s - loss: 0.38 - ETA: 10s - loss: 0.39 - ETA: 9s - loss: 0.3864 - ETA: 7s - loss: 0.387 - ETA: 6s - loss: 0.391 - ETA: 4s - loss: 0.389 - ETA: 3s - loss: 0.389 - ETA: 1s - loss: 0.388 - ETA: 0s - loss: 0.391 - 20s 3ms/step - loss: 0.3901 - val_loss: 0.4114\n",
      "Epoch 40/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.40 - ETA: 14s - loss: 0.39 - ETA: 13s - loss: 0.39 - ETA: 12s - loss: 0.39 - ETA: 11s - loss: 0.40 - ETA: 10s - loss: 0.40 - ETA: 9s - loss: 0.3987 - ETA: 7s - loss: 0.400 - ETA: 6s - loss: 0.398 - ETA: 4s - loss: 0.396 - ETA: 3s - loss: 0.395 - ETA: 1s - loss: 0.393 - ETA: 0s - loss: 0.391 - 20s 3ms/step - loss: 0.3913 - val_loss: 0.4163\n",
      "Epoch 41/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.37 - ETA: 17s - loss: 0.38 - ETA: 15s - loss: 0.38 - ETA: 13s - loss: 0.39 - ETA: 12s - loss: 0.39 - ETA: 10s - loss: 0.39 - ETA: 9s - loss: 0.3941 - ETA: 7s - loss: 0.393 - ETA: 6s - loss: 0.393 - ETA: 4s - loss: 0.394 - ETA: 3s - loss: 0.393 - ETA: 1s - loss: 0.392 - ETA: 0s - loss: 0.389 - 20s 3ms/step - loss: 0.3885 - val_loss: 0.4211\n",
      "Epoch 42/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.40 - ETA: 16s - loss: 0.39 - ETA: 14s - loss: 0.40 - ETA: 13s - loss: 0.39 - ETA: 12s - loss: 0.38 - ETA: 10s - loss: 0.39 - ETA: 9s - loss: 0.3933 - ETA: 7s - loss: 0.387 - ETA: 6s - loss: 0.380 - ETA: 4s - loss: 0.380 - ETA: 3s - loss: 0.380 - ETA: 1s - loss: 0.382 - ETA: 0s - loss: 0.381 - 20s 3ms/step - loss: 0.3817 - val_loss: 0.4091\n",
      "Epoch 43/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.34 - ETA: 16s - loss: 0.37 - ETA: 14s - loss: 0.37 - ETA: 13s - loss: 0.37 - ETA: 11s - loss: 0.38 - ETA: 10s - loss: 0.37 - ETA: 8s - loss: 0.3745 - ETA: 7s - loss: 0.373 - ETA: 6s - loss: 0.372 - ETA: 4s - loss: 0.374 - ETA: 3s - loss: 0.377 - ETA: 1s - loss: 0.376 - ETA: 0s - loss: 0.377 - 20s 3ms/step - loss: 0.3791 - val_loss: 0.4065\n",
      "Epoch 44/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.39 - ETA: 16s - loss: 0.40 - ETA: 14s - loss: 0.39 - ETA: 13s - loss: 0.39 - ETA: 11s - loss: 0.39 - ETA: 10s - loss: 0.38 - ETA: 9s - loss: 0.3875 - ETA: 7s - loss: 0.385 - ETA: 6s - loss: 0.381 - ETA: 4s - loss: 0.384 - ETA: 3s - loss: 0.381 - ETA: 1s - loss: 0.382 - ETA: 0s - loss: 0.383 - 19s 3ms/step - loss: 0.3842 - val_loss: 0.4160\n",
      "Epoch 45/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.31 - ETA: 17s - loss: 0.32 - ETA: 16s - loss: 0.33 - ETA: 14s - loss: 0.34 - ETA: 13s - loss: 0.34 - ETA: 11s - loss: 0.35 - ETA: 10s - loss: 0.35 - ETA: 8s - loss: 0.3661 - ETA: 7s - loss: 0.366 - ETA: 5s - loss: 0.366 - ETA: 3s - loss: 0.366 - ETA: 2s - loss: 0.366 - ETA: 0s - loss: 0.371 - 22s 3ms/step - loss: 0.3709 - val_loss: 0.4089\n",
      "Epoch 46/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.37 - ETA: 17s - loss: 0.37 - ETA: 15s - loss: 0.37 - ETA: 14s - loss: 0.37 - ETA: 12s - loss: 0.37 - ETA: 11s - loss: 0.36 - ETA: 9s - loss: 0.3670 - ETA: 8s - loss: 0.357 - ETA: 6s - loss: 0.358 - ETA: 5s - loss: 0.360 - ETA: 3s - loss: 0.358 - ETA: 2s - loss: 0.359 - ETA: 0s - loss: 0.362 - 22s 3ms/step - loss: 0.3637 - val_loss: 0.4183\n",
      "Epoch 47/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.37 - ETA: 17s - loss: 0.37 - ETA: 16s - loss: 0.37 - ETA: 15s - loss: 0.37 - ETA: 13s - loss: 0.37 - ETA: 11s - loss: 0.37 - ETA: 10s - loss: 0.37 - ETA: 8s - loss: 0.3812 - ETA: 6s - loss: 0.381 - ETA: 5s - loss: 0.376 - ETA: 3s - loss: 0.374 - ETA: 2s - loss: 0.374 - ETA: 0s - loss: 0.374 - 22s 3ms/step - loss: 0.3722 - val_loss: 0.4179\n",
      "Epoch 48/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.32 - ETA: 17s - loss: 0.35 - ETA: 16s - loss: 0.36 - ETA: 14s - loss: 0.36 - ETA: 13s - loss: 0.36 - ETA: 11s - loss: 0.37 - ETA: 10s - loss: 0.36 - ETA: 8s - loss: 0.3716 - ETA: 6s - loss: 0.366 - ETA: 5s - loss: 0.367 - ETA: 3s - loss: 0.368 - ETA: 2s - loss: 0.369 - ETA: 0s - loss: 0.369 - 22s 3ms/step - loss: 0.3690 - val_loss: 0.4327\n",
      "Epoch 49/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.31 - ETA: 17s - loss: 0.35 - ETA: 16s - loss: 0.35 - ETA: 14s - loss: 0.35 - ETA: 13s - loss: 0.36 - ETA: 11s - loss: 0.34 - ETA: 9s - loss: 0.3575 - ETA: 8s - loss: 0.358 - ETA: 6s - loss: 0.355 - ETA: 5s - loss: 0.353 - ETA: 3s - loss: 0.359 - ETA: 2s - loss: 0.358 - ETA: 0s - loss: 0.362 - 22s 3ms/step - loss: 0.3627 - val_loss: 0.4206\n",
      "Epoch 50/100\n",
      "6851/6851 [==============================] - ETA: 21s - loss: 0.35 - ETA: 19s - loss: 0.35 - ETA: 17s - loss: 0.35 - ETA: 15s - loss: 0.35 - ETA: 13s - loss: 0.35 - ETA: 11s - loss: 0.35 - ETA: 10s - loss: 0.35 - ETA: 8s - loss: 0.3511 - ETA: 6s - loss: 0.356 - ETA: 5s - loss: 0.352 - ETA: 3s - loss: 0.355 - ETA: 2s - loss: 0.357 - ETA: 0s - loss: 0.358 - 22s 3ms/step - loss: 0.3567 - val_loss: 0.4191\n",
      "Epoch 51/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.33 - ETA: 18s - loss: 0.36 - ETA: 16s - loss: 0.36 - ETA: 15s - loss: 0.34 - ETA: 13s - loss: 0.35 - ETA: 12s - loss: 0.35 - ETA: 10s - loss: 0.35 - ETA: 8s - loss: 0.3593 - ETA: 7s - loss: 0.357 - ETA: 5s - loss: 0.359 - ETA: 3s - loss: 0.360 - ETA: 2s - loss: 0.363 - ETA: 0s - loss: 0.362 - 22s 3ms/step - loss: 0.3651 - val_loss: 0.4336\n",
      "Epoch 52/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.33 - ETA: 18s - loss: 0.37 - ETA: 16s - loss: 0.36 - ETA: 14s - loss: 0.36 - ETA: 13s - loss: 0.36 - ETA: 11s - loss: 0.36 - ETA: 10s - loss: 0.36 - ETA: 8s - loss: 0.3585 - ETA: 6s - loss: 0.357 - ETA: 5s - loss: 0.356 - ETA: 3s - loss: 0.355 - ETA: 2s - loss: 0.355 - ETA: 0s - loss: 0.354 - 22s 3ms/step - loss: 0.3543 - val_loss: 0.4171\n",
      "Epoch 53/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.34 - ETA: 17s - loss: 0.35 - ETA: 15s - loss: 0.36 - ETA: 14s - loss: 0.36 - ETA: 13s - loss: 0.35 - ETA: 11s - loss: 0.35 - ETA: 9s - loss: 0.3569 - ETA: 8s - loss: 0.353 - ETA: 6s - loss: 0.354 - ETA: 5s - loss: 0.354 - ETA: 3s - loss: 0.352 - ETA: 2s - loss: 0.354 - ETA: 0s - loss: 0.354 - 22s 3ms/step - loss: 0.3545 - val_loss: 0.4206\n",
      "Epoch 54/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.34 - ETA: 17s - loss: 0.34 - ETA: 16s - loss: 0.36 - ETA: 14s - loss: 0.36 - ETA: 13s - loss: 0.36 - ETA: 11s - loss: 0.36 - ETA: 9s - loss: 0.3560 - ETA: 8s - loss: 0.359 - ETA: 6s - loss: 0.357 - ETA: 5s - loss: 0.356 - ETA: 3s - loss: 0.357 - ETA: 2s - loss: 0.357 - ETA: 0s - loss: 0.356 - 22s 3ms/step - loss: 0.3542 - val_loss: 0.4169\n",
      "Epoch 55/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.35 - ETA: 18s - loss: 0.37 - ETA: 16s - loss: 0.37 - ETA: 15s - loss: 0.36 - ETA: 13s - loss: 0.36 - ETA: 12s - loss: 0.35 - ETA: 10s - loss: 0.35 - ETA: 8s - loss: 0.3502 - ETA: 7s - loss: 0.346 - ETA: 5s - loss: 0.343 - ETA: 3s - loss: 0.344 - ETA: 2s - loss: 0.342 - ETA: 0s - loss: 0.345 - 23s 3ms/step - loss: 0.3466 - val_loss: 0.4341\n",
      "Epoch 56/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.37 - ETA: 18s - loss: 0.36 - ETA: 17s - loss: 0.35 - ETA: 15s - loss: 0.34 - ETA: 13s - loss: 0.35 - ETA: 11s - loss: 0.35 - ETA: 10s - loss: 0.35 - ETA: 8s - loss: 0.3444 - ETA: 7s - loss: 0.345 - ETA: 5s - loss: 0.343 - ETA: 3s - loss: 0.344 - ETA: 2s - loss: 0.340 - ETA: 0s - loss: 0.342 - 23s 3ms/step - loss: 0.3410 - val_loss: 0.4479\n",
      "Epoch 57/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.36 - ETA: 19s - loss: 0.33 - ETA: 17s - loss: 0.33 - ETA: 15s - loss: 0.34 - ETA: 13s - loss: 0.34 - ETA: 11s - loss: 0.34 - ETA: 10s - loss: 0.34 - ETA: 8s - loss: 0.3430 - ETA: 7s - loss: 0.348 - ETA: 5s - loss: 0.345 - ETA: 3s - loss: 0.346 - ETA: 2s - loss: 0.347 - ETA: 0s - loss: 0.348 - 23s 3ms/step - loss: 0.3467 - val_loss: 0.4271\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6851/6851 [==============================] - ETA: 19s - loss: 0.30 - ETA: 17s - loss: 0.32 - ETA: 16s - loss: 0.34 - ETA: 15s - loss: 0.34 - ETA: 13s - loss: 0.33 - ETA: 11s - loss: 0.33 - ETA: 10s - loss: 0.33 - ETA: 8s - loss: 0.3340 - ETA: 7s - loss: 0.338 - ETA: 5s - loss: 0.337 - ETA: 3s - loss: 0.334 - ETA: 2s - loss: 0.333 - ETA: 0s - loss: 0.335 - 22s 3ms/step - loss: 0.3355 - val_loss: 0.4294\n",
      "Epoch 59/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.34 - ETA: 17s - loss: 0.33 - ETA: 15s - loss: 0.32 - ETA: 14s - loss: 0.32 - ETA: 12s - loss: 0.33 - ETA: 11s - loss: 0.33 - ETA: 9s - loss: 0.3359 - ETA: 8s - loss: 0.341 - ETA: 6s - loss: 0.337 - ETA: 5s - loss: 0.340 - ETA: 3s - loss: 0.339 - ETA: 2s - loss: 0.337 - ETA: 0s - loss: 0.337 - 22s 3ms/step - loss: 0.3361 - val_loss: 0.4422\n",
      "Epoch 60/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.33 - ETA: 16s - loss: 0.34 - ETA: 15s - loss: 0.33 - ETA: 14s - loss: 0.32 - ETA: 12s - loss: 0.32 - ETA: 11s - loss: 0.33 - ETA: 9s - loss: 0.3396 - ETA: 8s - loss: 0.338 - ETA: 6s - loss: 0.338 - ETA: 5s - loss: 0.336 - ETA: 3s - loss: 0.339 - ETA: 2s - loss: 0.339 - ETA: 0s - loss: 0.339 - 21s 3ms/step - loss: 0.3372 - val_loss: 0.4282\n",
      "Epoch 61/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.31 - ETA: 17s - loss: 0.33 - ETA: 16s - loss: 0.33 - ETA: 14s - loss: 0.33 - ETA: 13s - loss: 0.33 - ETA: 11s - loss: 0.33 - ETA: 10s - loss: 0.33 - ETA: 8s - loss: 0.3374 - ETA: 6s - loss: 0.338 - ETA: 5s - loss: 0.333 - ETA: 3s - loss: 0.331 - ETA: 2s - loss: 0.331 - ETA: 0s - loss: 0.330 - 22s 3ms/step - loss: 0.3308 - val_loss: 0.4289\n",
      "Epoch 62/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.32 - ETA: 17s - loss: 0.32 - ETA: 16s - loss: 0.31 - ETA: 14s - loss: 0.31 - ETA: 13s - loss: 0.32 - ETA: 11s - loss: 0.31 - ETA: 10s - loss: 0.32 - ETA: 8s - loss: 0.3187 - ETA: 7s - loss: 0.320 - ETA: 5s - loss: 0.324 - ETA: 3s - loss: 0.322 - ETA: 2s - loss: 0.325 - ETA: 0s - loss: 0.325 - 22s 3ms/step - loss: 0.3249 - val_loss: 0.4220\n",
      "Epoch 63/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.33 - ETA: 18s - loss: 0.33 - ETA: 16s - loss: 0.33 - ETA: 14s - loss: 0.34 - ETA: 13s - loss: 0.34 - ETA: 11s - loss: 0.33 - ETA: 10s - loss: 0.33 - ETA: 8s - loss: 0.3400 - ETA: 6s - loss: 0.336 - ETA: 5s - loss: 0.333 - ETA: 3s - loss: 0.333 - ETA: 2s - loss: 0.333 - ETA: 0s - loss: 0.332 - 22s 3ms/step - loss: 0.3319 - val_loss: 0.4417\n",
      "Epoch 64/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.29 - ETA: 16s - loss: 0.32 - ETA: 15s - loss: 0.32 - ETA: 13s - loss: 0.32 - ETA: 12s - loss: 0.31 - ETA: 11s - loss: 0.31 - ETA: 9s - loss: 0.3178 - ETA: 8s - loss: 0.316 - ETA: 6s - loss: 0.319 - ETA: 5s - loss: 0.322 - ETA: 3s - loss: 0.319 - ETA: 2s - loss: 0.320 - ETA: 0s - loss: 0.323 - 22s 3ms/step - loss: 0.3244 - val_loss: 0.4304\n",
      "Epoch 65/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.34 - ETA: 17s - loss: 0.33 - ETA: 16s - loss: 0.33 - ETA: 15s - loss: 0.34 - ETA: 13s - loss: 0.34 - ETA: 11s - loss: 0.33 - ETA: 10s - loss: 0.33 - ETA: 8s - loss: 0.3384 - ETA: 7s - loss: 0.337 - ETA: 5s - loss: 0.332 - ETA: 3s - loss: 0.334 - ETA: 2s - loss: 0.334 - ETA: 0s - loss: 0.334 - 21s 3ms/step - loss: 0.3340 - val_loss: 0.4356\n",
      "Epoch 66/100\n",
      "6851/6851 [==============================] - ETA: 12s - loss: 0.27 - ETA: 11s - loss: 0.32 - ETA: 10s - loss: 0.31 - ETA: 9s - loss: 0.3153 - ETA: 8s - loss: 0.311 - ETA: 7s - loss: 0.306 - ETA: 6s - loss: 0.308 - ETA: 5s - loss: 0.308 - ETA: 4s - loss: 0.310 - ETA: 3s - loss: 0.313 - ETA: 2s - loss: 0.315 - ETA: 1s - loss: 0.317 - ETA: 0s - loss: 0.319 - 15s 2ms/step - loss: 0.3175 - val_loss: 0.4435\n",
      "Epoch 67/100\n",
      "6851/6851 [==============================] - ETA: 13s - loss: 0.29 - ETA: 12s - loss: 0.31 - ETA: 11s - loss: 0.31 - ETA: 10s - loss: 0.31 - ETA: 9s - loss: 0.3105 - ETA: 8s - loss: 0.307 - ETA: 7s - loss: 0.304 - ETA: 6s - loss: 0.309 - ETA: 5s - loss: 0.316 - ETA: 4s - loss: 0.315 - ETA: 2s - loss: 0.316 - ETA: 1s - loss: 0.314 - ETA: 0s - loss: 0.317 - 17s 3ms/step - loss: 0.3168 - val_loss: 0.4546\n",
      "Epoch 68/100\n",
      "6851/6851 [==============================] - ETA: 14s - loss: 0.34 - ETA: 13s - loss: 0.32 - ETA: 12s - loss: 0.31 - ETA: 11s - loss: 0.31 - ETA: 10s - loss: 0.30 - ETA: 9s - loss: 0.3027 - ETA: 8s - loss: 0.307 - ETA: 7s - loss: 0.309 - ETA: 5s - loss: 0.308 - ETA: 4s - loss: 0.305 - ETA: 3s - loss: 0.308 - ETA: 1s - loss: 0.307 - ETA: 0s - loss: 0.305 - 18s 3ms/step - loss: 0.3068 - val_loss: 0.4407\n",
      "Epoch 69/100\n",
      "6851/6851 [==============================] - ETA: 14s - loss: 0.35 - ETA: 13s - loss: 0.31 - ETA: 12s - loss: 0.31 - ETA: 11s - loss: 0.32 - ETA: 10s - loss: 0.31 - ETA: 9s - loss: 0.3149 - ETA: 8s - loss: 0.316 - ETA: 6s - loss: 0.315 - ETA: 5s - loss: 0.312 - ETA: 4s - loss: 0.314 - ETA: 3s - loss: 0.312 - ETA: 1s - loss: 0.310 - ETA: 0s - loss: 0.309 - 18s 3ms/step - loss: 0.3092 - val_loss: 0.4562\n",
      "Epoch 70/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.28 - ETA: 17s - loss: 0.29 - ETA: 15s - loss: 0.30 - ETA: 13s - loss: 0.31 - ETA: 11s - loss: 0.30 - ETA: 9s - loss: 0.3090 - ETA: 8s - loss: 0.312 - ETA: 7s - loss: 0.310 - ETA: 5s - loss: 0.309 - ETA: 4s - loss: 0.306 - ETA: 3s - loss: 0.306 - ETA: 1s - loss: 0.309 - ETA: 0s - loss: 0.307 - 17s 3ms/step - loss: 0.3079 - val_loss: 0.4750\n",
      "Epoch 71/100\n",
      "6851/6851 [==============================] - ETA: 14s - loss: 0.29 - ETA: 13s - loss: 0.31 - ETA: 11s - loss: 0.30 - ETA: 11s - loss: 0.30 - ETA: 10s - loss: 0.30 - ETA: 8s - loss: 0.3064 - ETA: 7s - loss: 0.306 - ETA: 6s - loss: 0.305 - ETA: 5s - loss: 0.305 - ETA: 4s - loss: 0.303 - ETA: 2s - loss: 0.304 - ETA: 1s - loss: 0.307 - ETA: 0s - loss: 0.310 - 17s 2ms/step - loss: 0.3110 - val_loss: 0.4522\n",
      "Epoch 72/100\n",
      "6851/6851 [==============================] - ETA: 15s - loss: 0.29 - ETA: 13s - loss: 0.30 - ETA: 12s - loss: 0.30 - ETA: 10s - loss: 0.30 - ETA: 9s - loss: 0.3057 - ETA: 8s - loss: 0.302 - ETA: 7s - loss: 0.296 - ETA: 6s - loss: 0.297 - ETA: 5s - loss: 0.298 - ETA: 4s - loss: 0.293 - ETA: 3s - loss: 0.299 - ETA: 1s - loss: 0.298 - ETA: 0s - loss: 0.297 - 18s 3ms/step - loss: 0.2973 - val_loss: 0.4538\n",
      "Epoch 73/100\n",
      "6851/6851 [==============================] - ETA: 14s - loss: 0.30 - ETA: 13s - loss: 0.31 - ETA: 11s - loss: 0.31 - ETA: 10s - loss: 0.31 - ETA: 9s - loss: 0.3160 - ETA: 8s - loss: 0.322 - ETA: 7s - loss: 0.317 - ETA: 6s - loss: 0.309 - ETA: 5s - loss: 0.304 - ETA: 4s - loss: 0.300 - ETA: 3s - loss: 0.301 - ETA: 1s - loss: 0.296 - ETA: 0s - loss: 0.299 - 20s 3ms/step - loss: 0.2981 - val_loss: 0.4783\n",
      "Epoch 74/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.29 - ETA: 17s - loss: 0.28 - ETA: 15s - loss: 0.30 - ETA: 12s - loss: 0.28 - ETA: 11s - loss: 0.29 - ETA: 9s - loss: 0.2915 - ETA: 8s - loss: 0.290 - ETA: 6s - loss: 0.295 - ETA: 5s - loss: 0.290 - ETA: 4s - loss: 0.291 - ETA: 3s - loss: 0.289 - ETA: 1s - loss: 0.289 - ETA: 0s - loss: 0.288 - 18s 3ms/step - loss: 0.2891 - val_loss: 0.4713\n",
      "Epoch 75/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.26 - ETA: 14s - loss: 0.28 - ETA: 13s - loss: 0.28 - ETA: 12s - loss: 0.30 - ETA: 11s - loss: 0.30 - ETA: 9s - loss: 0.2979 - ETA: 8s - loss: 0.293 - ETA: 7s - loss: 0.293 - ETA: 5s - loss: 0.297 - ETA: 4s - loss: 0.296 - ETA: 3s - loss: 0.299 - ETA: 1s - loss: 0.297 - ETA: 0s - loss: 0.298 - 19s 3ms/step - loss: 0.2994 - val_loss: 0.4650\n",
      "Epoch 76/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.31 - ETA: 15s - loss: 0.31 - ETA: 14s - loss: 0.31 - ETA: 12s - loss: 0.30 - ETA: 11s - loss: 0.30 - ETA: 10s - loss: 0.30 - ETA: 8s - loss: 0.3077 - ETA: 7s - loss: 0.307 - ETA: 6s - loss: 0.302 - ETA: 4s - loss: 0.300 - ETA: 3s - loss: 0.297 - ETA: 1s - loss: 0.298 - ETA: 0s - loss: 0.301 - 20s 3ms/step - loss: 0.2995 - val_loss: 0.4753\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6851/6851 [==============================] - ETA: 17s - loss: 0.31 - ETA: 15s - loss: 0.31 - ETA: 13s - loss: 0.33 - ETA: 12s - loss: 0.30 - ETA: 11s - loss: 0.29 - ETA: 9s - loss: 0.2977 - ETA: 8s - loss: 0.296 - ETA: 7s - loss: 0.298 - ETA: 5s - loss: 0.297 - ETA: 4s - loss: 0.295 - ETA: 3s - loss: 0.295 - ETA: 1s - loss: 0.291 - ETA: 0s - loss: 0.294 - 19s 3ms/step - loss: 0.2961 - val_loss: 0.4610\n",
      "Epoch 78/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.32 - ETA: 14s - loss: 0.31 - ETA: 13s - loss: 0.29 - ETA: 12s - loss: 0.29 - ETA: 11s - loss: 0.29 - ETA: 9s - loss: 0.2910 - ETA: 8s - loss: 0.287 - ETA: 7s - loss: 0.289 - ETA: 5s - loss: 0.286 - ETA: 4s - loss: 0.290 - ETA: 3s - loss: 0.287 - ETA: 1s - loss: 0.287 - ETA: 0s - loss: 0.289 - 19s 3ms/step - loss: 0.2894 - val_loss: 0.4702\n",
      "Epoch 79/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.29 - ETA: 15s - loss: 0.28 - ETA: 14s - loss: 0.29 - ETA: 12s - loss: 0.28 - ETA: 11s - loss: 0.28 - ETA: 10s - loss: 0.28 - ETA: 8s - loss: 0.2850 - ETA: 7s - loss: 0.281 - ETA: 6s - loss: 0.287 - ETA: 4s - loss: 0.286 - ETA: 3s - loss: 0.283 - ETA: 1s - loss: 0.283 - ETA: 0s - loss: 0.282 - 19s 3ms/step - loss: 0.2826 - val_loss: 0.4634\n",
      "Epoch 80/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.28 - ETA: 15s - loss: 0.27 - ETA: 13s - loss: 0.27 - ETA: 12s - loss: 0.27 - ETA: 11s - loss: 0.27 - ETA: 10s - loss: 0.27 - ETA: 8s - loss: 0.2781 - ETA: 7s - loss: 0.277 - ETA: 5s - loss: 0.279 - ETA: 4s - loss: 0.280 - ETA: 3s - loss: 0.275 - ETA: 1s - loss: 0.276 - ETA: 0s - loss: 0.274 - 19s 3ms/step - loss: 0.2721 - val_loss: 0.4950\n",
      "Epoch 81/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.30 - ETA: 16s - loss: 0.29 - ETA: 14s - loss: 0.29 - ETA: 12s - loss: 0.28 - ETA: 11s - loss: 0.28 - ETA: 9s - loss: 0.2856 - ETA: 8s - loss: 0.281 - ETA: 7s - loss: 0.283 - ETA: 5s - loss: 0.280 - ETA: 4s - loss: 0.276 - ETA: 3s - loss: 0.278 - ETA: 1s - loss: 0.280 - ETA: 0s - loss: 0.281 - 19s 3ms/step - loss: 0.2824 - val_loss: 0.5007\n",
      "Epoch 82/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.25 - ETA: 16s - loss: 0.24 - ETA: 14s - loss: 0.26 - ETA: 13s - loss: 0.26 - ETA: 12s - loss: 0.26 - ETA: 10s - loss: 0.26 - ETA: 8s - loss: 0.2675 - ETA: 7s - loss: 0.267 - ETA: 6s - loss: 0.267 - ETA: 4s - loss: 0.273 - ETA: 3s - loss: 0.275 - ETA: 1s - loss: 0.275 - ETA: 0s - loss: 0.275 - 19s 3ms/step - loss: 0.2760 - val_loss: 0.4806\n",
      "Epoch 83/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.29 - ETA: 15s - loss: 0.27 - ETA: 14s - loss: 0.25 - ETA: 12s - loss: 0.25 - ETA: 11s - loss: 0.26 - ETA: 9s - loss: 0.2565 - ETA: 8s - loss: 0.262 - ETA: 7s - loss: 0.264 - ETA: 5s - loss: 0.264 - ETA: 4s - loss: 0.265 - ETA: 3s - loss: 0.268 - ETA: 1s - loss: 0.274 - ETA: 0s - loss: 0.275 - 19s 3ms/step - loss: 0.2752 - val_loss: 0.4775\n",
      "Epoch 84/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.23 - ETA: 15s - loss: 0.26 - ETA: 13s - loss: 0.27 - ETA: 12s - loss: 0.25 - ETA: 11s - loss: 0.25 - ETA: 10s - loss: 0.26 - ETA: 8s - loss: 0.2621 - ETA: 7s - loss: 0.260 - ETA: 5s - loss: 0.263 - ETA: 4s - loss: 0.263 - ETA: 3s - loss: 0.265 - ETA: 1s - loss: 0.262 - ETA: 0s - loss: 0.265 - 19s 3ms/step - loss: 0.2648 - val_loss: 0.5005\n",
      "Epoch 85/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.25 - ETA: 15s - loss: 0.27 - ETA: 14s - loss: 0.28 - ETA: 12s - loss: 0.28 - ETA: 11s - loss: 0.28 - ETA: 10s - loss: 0.28 - ETA: 8s - loss: 0.2745 - ETA: 7s - loss: 0.267 - ETA: 6s - loss: 0.271 - ETA: 4s - loss: 0.273 - ETA: 3s - loss: 0.278 - ETA: 1s - loss: 0.276 - ETA: 0s - loss: 0.276 - 20s 3ms/step - loss: 0.2779 - val_loss: 0.5150\n",
      "Epoch 86/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.25 - ETA: 16s - loss: 0.27 - ETA: 14s - loss: 0.26 - ETA: 13s - loss: 0.26 - ETA: 11s - loss: 0.26 - ETA: 10s - loss: 0.27 - ETA: 9s - loss: 0.2682 - ETA: 7s - loss: 0.267 - ETA: 6s - loss: 0.266 - ETA: 4s - loss: 0.264 - ETA: 3s - loss: 0.260 - ETA: 1s - loss: 0.261 - ETA: 0s - loss: 0.266 - 20s 3ms/step - loss: 0.2651 - val_loss: 0.4955\n",
      "Epoch 87/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.24 - ETA: 16s - loss: 0.24 - ETA: 15s - loss: 0.24 - ETA: 14s - loss: 0.24 - ETA: 12s - loss: 0.24 - ETA: 10s - loss: 0.24 - ETA: 9s - loss: 0.2510 - ETA: 7s - loss: 0.253 - ETA: 6s - loss: 0.262 - ETA: 5s - loss: 0.265 - ETA: 3s - loss: 0.267 - ETA: 2s - loss: 0.266 - ETA: 0s - loss: 0.267 - 22s 3ms/step - loss: 0.2676 - val_loss: 0.4862\n",
      "Epoch 88/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.30 - ETA: 16s - loss: 0.27 - ETA: 14s - loss: 0.26 - ETA: 13s - loss: 0.26 - ETA: 12s - loss: 0.26 - ETA: 10s - loss: 0.26 - ETA: 9s - loss: 0.2634 - ETA: 7s - loss: 0.259 - ETA: 6s - loss: 0.262 - ETA: 4s - loss: 0.261 - ETA: 3s - loss: 0.266 - ETA: 1s - loss: 0.262 - ETA: 0s - loss: 0.262 - 20s 3ms/step - loss: 0.2621 - val_loss: 0.4835\n",
      "Epoch 89/100\n",
      "6851/6851 [==============================] - ETA: 17s - loss: 0.25 - ETA: 15s - loss: 0.25 - ETA: 14s - loss: 0.25 - ETA: 13s - loss: 0.25 - ETA: 11s - loss: 0.25 - ETA: 10s - loss: 0.25 - ETA: 8s - loss: 0.2594 - ETA: 7s - loss: 0.256 - ETA: 6s - loss: 0.254 - ETA: 4s - loss: 0.256 - ETA: 3s - loss: 0.252 - ETA: 1s - loss: 0.249 - ETA: 0s - loss: 0.250 - 20s 3ms/step - loss: 0.2522 - val_loss: 0.5043\n",
      "Epoch 90/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.22 - ETA: 17s - loss: 0.21 - ETA: 16s - loss: 0.22 - ETA: 14s - loss: 0.23 - ETA: 12s - loss: 0.23 - ETA: 11s - loss: 0.25 - ETA: 9s - loss: 0.2522 - ETA: 8s - loss: 0.251 - ETA: 6s - loss: 0.255 - ETA: 5s - loss: 0.255 - ETA: 3s - loss: 0.254 - ETA: 2s - loss: 0.255 - ETA: 0s - loss: 0.256 - 21s 3ms/step - loss: 0.2574 - val_loss: 0.4977\n",
      "Epoch 91/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.22 - ETA: 15s - loss: 0.24 - ETA: 13s - loss: 0.23 - ETA: 12s - loss: 0.24 - ETA: 11s - loss: 0.24 - ETA: 10s - loss: 0.24 - ETA: 8s - loss: 0.2529 - ETA: 7s - loss: 0.256 - ETA: 6s - loss: 0.258 - ETA: 4s - loss: 0.262 - ETA: 3s - loss: 0.264 - ETA: 1s - loss: 0.264 - ETA: 0s - loss: 0.264 - 20s 3ms/step - loss: 0.2648 - val_loss: 0.4829\n",
      "Epoch 92/100\n",
      "6851/6851 [==============================] - ETA: 16s - loss: 0.26 - ETA: 15s - loss: 0.27 - ETA: 14s - loss: 0.27 - ETA: 13s - loss: 0.26 - ETA: 11s - loss: 0.27 - ETA: 10s - loss: 0.26 - ETA: 8s - loss: 0.2676 - ETA: 7s - loss: 0.267 - ETA: 6s - loss: 0.261 - ETA: 4s - loss: 0.259 - ETA: 3s - loss: 0.256 - ETA: 1s - loss: 0.257 - ETA: 0s - loss: 0.263 - 20s 3ms/step - loss: 0.2646 - val_loss: 0.5077\n",
      "Epoch 93/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.26 - ETA: 17s - loss: 0.25 - ETA: 15s - loss: 0.26 - ETA: 14s - loss: 0.26 - ETA: 12s - loss: 0.26 - ETA: 11s - loss: 0.26 - ETA: 9s - loss: 0.2585 - ETA: 8s - loss: 0.260 - ETA: 6s - loss: 0.258 - ETA: 5s - loss: 0.255 - ETA: 3s - loss: 0.256 - ETA: 2s - loss: 0.252 - ETA: 0s - loss: 0.252 - 21s 3ms/step - loss: 0.2544 - val_loss: 0.5337\n",
      "Epoch 94/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.23 - ETA: 17s - loss: 0.23 - ETA: 16s - loss: 0.24 - ETA: 14s - loss: 0.24 - ETA: 12s - loss: 0.24 - ETA: 11s - loss: 0.24 - ETA: 9s - loss: 0.2492 - ETA: 8s - loss: 0.247 - ETA: 6s - loss: 0.252 - ETA: 5s - loss: 0.250 - ETA: 3s - loss: 0.252 - ETA: 2s - loss: 0.249 - ETA: 0s - loss: 0.248 - 22s 3ms/step - loss: 0.2480 - val_loss: 0.5210\n",
      "Epoch 95/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.22 - ETA: 17s - loss: 0.23 - ETA: 15s - loss: 0.24 - ETA: 14s - loss: 0.25 - ETA: 12s - loss: 0.25 - ETA: 11s - loss: 0.24 - ETA: 9s - loss: 0.2426 - ETA: 8s - loss: 0.240 - ETA: 6s - loss: 0.244 - ETA: 5s - loss: 0.248 - ETA: 3s - loss: 0.248 - ETA: 2s - loss: 0.246 - ETA: 0s - loss: 0.250 - 22s 3ms/step - loss: 0.2511 - val_loss: 0.4918\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6851/6851 [==============================] - ETA: 18s - loss: 0.26 - ETA: 17s - loss: 0.27 - ETA: 16s - loss: 0.26 - ETA: 14s - loss: 0.26 - ETA: 13s - loss: 0.26 - ETA: 11s - loss: 0.25 - ETA: 10s - loss: 0.25 - ETA: 8s - loss: 0.2597 - ETA: 6s - loss: 0.255 - ETA: 5s - loss: 0.252 - ETA: 3s - loss: 0.250 - ETA: 2s - loss: 0.247 - ETA: 0s - loss: 0.249 - 22s 3ms/step - loss: 0.2480 - val_loss: 0.5501\n",
      "Epoch 97/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.22 - ETA: 18s - loss: 0.21 - ETA: 16s - loss: 0.22 - ETA: 14s - loss: 0.22 - ETA: 13s - loss: 0.23 - ETA: 11s - loss: 0.23 - ETA: 10s - loss: 0.23 - ETA: 8s - loss: 0.2360 - ETA: 7s - loss: 0.237 - ETA: 5s - loss: 0.240 - ETA: 3s - loss: 0.239 - ETA: 2s - loss: 0.238 - ETA: 0s - loss: 0.239 - 23s 3ms/step - loss: 0.2411 - val_loss: 0.5239\n",
      "Epoch 98/100\n",
      "6851/6851 [==============================] - ETA: 19s - loss: 0.22 - ETA: 17s - loss: 0.22 - ETA: 16s - loss: 0.23 - ETA: 15s - loss: 0.23 - ETA: 13s - loss: 0.24 - ETA: 11s - loss: 0.24 - ETA: 10s - loss: 0.23 - ETA: 8s - loss: 0.2358 - ETA: 7s - loss: 0.234 - ETA: 5s - loss: 0.235 - ETA: 3s - loss: 0.242 - ETA: 2s - loss: 0.241 - ETA: 0s - loss: 0.240 - 22s 3ms/step - loss: 0.2421 - val_loss: 0.5290\n",
      "Epoch 99/100\n",
      "6851/6851 [==============================] - ETA: 20s - loss: 0.25 - ETA: 19s - loss: 0.25 - ETA: 16s - loss: 0.25 - ETA: 15s - loss: 0.25 - ETA: 13s - loss: 0.25 - ETA: 11s - loss: 0.25 - ETA: 10s - loss: 0.25 - ETA: 8s - loss: 0.2475 - ETA: 6s - loss: 0.242 - ETA: 5s - loss: 0.245 - ETA: 3s - loss: 0.245 - ETA: 2s - loss: 0.242 - ETA: 0s - loss: 0.244 - 22s 3ms/step - loss: 0.2435 - val_loss: 0.5250\n",
      "Epoch 100/100\n",
      "6851/6851 [==============================] - ETA: 18s - loss: 0.24 - ETA: 17s - loss: 0.24 - ETA: 16s - loss: 0.23 - ETA: 15s - loss: 0.23 - ETA: 13s - loss: 0.23 - ETA: 11s - loss: 0.23 - ETA: 10s - loss: 0.23 - ETA: 8s - loss: 0.2319 - ETA: 6s - loss: 0.230 - ETA: 5s - loss: 0.231 - ETA: 3s - loss: 0.232 - ETA: 2s - loss: 0.231 - ETA: 0s - loss: 0.230 - 22s 3ms/step - loss: 0.2316 - val_loss: 0.5319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a7c18ebf28>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleLSTM score: 0.8667158775352385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "predictions = model.predict(xvalid_pad)\n",
    "# print(classification_report(predictions,yvalid))\n",
    "score=roc_auc_score(yvalid,predictions[:,1])\n",
    "print(\"{} score: {}\".format(\"simpleLSTM\",score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "predictions=model.predict(xvalid_pad)\n",
    "thresholds=np.arange(0.1,1,0.05)\n",
    "for threshold in thresholds:\n",
    "    temp=predictions[:,1].copy()\n",
    "    temp[temp>=threshold]=1\n",
    "    temp[temp<threshold]=0\n",
    "    score=f1_score(yvalid,temp)\n",
    "    print(\"threshold {} score: {}\".format(threshold,score))\n",
    "threshold=0.25\n",
    "testpred=model.predict(xtest_pad)\n",
    "testpred=testpred[:,1].copy()\n",
    "testpred[testpred>=threshold]=1\n",
    "testpred[testpred<threshold]=0\n",
    "test[\"simplenn_0.25\"]=testpred\n",
    "test[\"simplenn_0.25\"]=test[\"simplenn_0.25\"].astype(int)\n",
    "test[[\"id\",\"simplenn_0.25\"]].rename(columns={\"simplenn_0.25\":\"target\"}).to_csv(\"./submissions/submission_nn_1.csv\",index=None)\n",
    "threshold=0.5\n",
    "testpred=model.predict(xtest_pad)\n",
    "testpred=testpred[:,1].copy()\n",
    "testpred[testpred>=threshold]=1\n",
    "testpred[testpred<threshold]=0\n",
    "test[\"simplenn_0.5\"]=testpred\n",
    "test[\"simplenn_0.5\"]=test[\"simplenn_0.5\"].astype(int)\n",
    "test[[\"id\",\"simplenn_0.5\"]].rename(columns={\"simplenn_0.5\":\"target\"}).to_csv(\"./submissions/submission_nn_2.csv\",index=None)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer bidirectional_1: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ad66d0a3bad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                      trainable=False))\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSpatialDropout1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyclegan\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    490\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[0;32m    491\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyclegan\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[1;31m# Standardize `initial_state` into list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyclegan\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyclegan\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    472\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    475\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer bidirectional_1: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "from keras.layers import Bidirectional\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model2.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model2.add(Dense(1024, activation='relu'))\n",
    "model2.add(Dropout(0.8))\n",
    "\n",
    "model2.add(Dense(1024, activation='relu'))\n",
    "model2.add(Dropout(0.8))\n",
    "\n",
    "model2.add(Dense(2))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model2.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.predict(xvalid_pad)\n",
    "# print(classification_report(predictions,yvalid))\n",
    "score=roc_auc_score(yvalid,predictions[:,1])\n",
    "print(\"{} score: {}\".format(\"BiLSTM\",score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/100\n",
      "6851/6851 [==============================] - ETA: 1:54 - loss: 0.699 - ETA: 1:32 - loss: 0.693 - ETA: 1:19 - loss: 0.689 - ETA: 1:10 - loss: 0.684 - ETA: 1:02 - loss: 0.680 - ETA: 55s - loss: 0.679 - ETA: 48s - loss: 0.67 - ETA: 40s - loss: 0.67 - ETA: 32s - loss: 0.66 - ETA: 25s - loss: 0.66 - ETA: 17s - loss: 0.66 - ETA: 10s - loss: 0.66 - ETA: 2s - loss: 0.6608 - 102s 15ms/step - loss: 0.6622 - val_loss: 0.6129\n",
      "Epoch 2/100\n",
      "6851/6851 [==============================] - ETA: 1:27 - loss: 0.638 - ETA: 1:23 - loss: 0.630 - ETA: 1:17 - loss: 0.629 - ETA: 1:09 - loss: 0.626 - ETA: 1:02 - loss: 0.619 - ETA: 55s - loss: 0.617 - ETA: 48s - loss: 0.60 - ETA: 40s - loss: 0.61 - ETA: 33s - loss: 0.60 - ETA: 25s - loss: 0.61 - ETA: 18s - loss: 0.60 - ETA: 10s - loss: 0.59 - ETA: 2s - loss: 0.5935 - 107s 16ms/step - loss: 0.5929 - val_loss: 0.4844\n",
      "Epoch 3/100\n",
      "6851/6851 [==============================] - ETA: 1:40 - loss: 0.521 - ETA: 1:30 - loss: 0.540 - ETA: 1:22 - loss: 0.537 - ETA: 1:14 - loss: 0.527 - ETA: 1:07 - loss: 0.526 - ETA: 58s - loss: 0.526 - ETA: 50s - loss: 0.51 - ETA: 43s - loss: 0.52 - ETA: 35s - loss: 0.52 - ETA: 27s - loss: 0.52 - ETA: 19s - loss: 0.52 - ETA: 11s - loss: 0.52 - ETA: 3s - loss: 0.5210 - 112s 16ms/step - loss: 0.5191 - val_loss: 0.4355\n",
      "Epoch 4/100\n",
      "3072/6851 [============>.................] - ETA: 1:37 - loss: 0.485 - ETA: 1:30 - loss: 0.489 - ETA: 1:23 - loss: 0.508 - ETA: 1:15 - loss: 0.505 - ETA: 1:07 - loss: 0.501 - ETA: 59s - loss: 0.5010 "
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model3.add(SpatialDropout1D(0.3))\n",
    "model3.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model3.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model3.add(Dense(1024, activation='relu'))\n",
    "model3.add(Dropout(0.8))\n",
    "\n",
    "model3.add(Dense(1024, activation='relu'))\n",
    "model3.add(Dropout(0.8))\n",
    "\n",
    "model3.add(Dense(2))\n",
    "model3.add(Activation('softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model3.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "predictions = model3.predict(xvalid_pad)\n",
    "# print(classification_report(predictions,yvalid))\n",
    "score=roc_auc_score(yvalid,predictions[:,1])\n",
    "print(\"{} score: {}\".format(\"GRU\",score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(model3,open(\"./models/model3.p\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
